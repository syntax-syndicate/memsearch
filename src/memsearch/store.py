"""Milvus vector storage layer using MilvusClient API."""

from __future__ import annotations

import logging
from pathlib import Path
from typing import Any

logger = logging.getLogger(__name__)


class MilvusStore:
    """Thin wrapper around ``pymilvus.MilvusClient`` for chunk storage.

    Creates collections with both dense vector and BM25 sparse vector
    fields, enabling hybrid search (semantic + keyword) by default.
    """

    DEFAULT_COLLECTION = "memsearch_chunks"

    def __init__(
        self,
        uri: str = "~/.memsearch/milvus.db",
        *,
        token: str | None = None,
        collection: str = DEFAULT_COLLECTION,
        dimension: int = 1536,
    ) -> None:
        from pymilvus import MilvusClient

        resolved = str(Path(uri).expanduser()) if not uri.startswith(("http", "tcp")) else uri
        if not uri.startswith(("http", "tcp")):
            Path(resolved).parent.mkdir(parents=True, exist_ok=True)
        connect_kwargs: dict[str, Any] = {"uri": resolved}
        if token:
            connect_kwargs["token"] = token
        self._client = MilvusClient(**connect_kwargs)
        self._collection = collection
        self._dimension = dimension
        self._ensure_collection()

    def _ensure_collection(self) -> None:
        if self._client.has_collection(self._collection):
            return

        from pymilvus import (
            CollectionSchema,
            DataType,
            FieldSchema,
            Function,
            FunctionType,
        )

        fields = [
            FieldSchema(
                name="chunk_hash", dtype=DataType.VARCHAR,
                max_length=64, is_primary=True,
            ),
            FieldSchema(
                name="embedding", dtype=DataType.FLOAT_VECTOR,
                dim=self._dimension,
            ),
            # BM25 full-text search: content field with analyzer enabled
            FieldSchema(
                name="content", dtype=DataType.VARCHAR,
                max_length=65535, enable_analyzer=True,
            ),
            # Sparse vector auto-populated by BM25 Function
            FieldSchema(
                name="sparse_vector", dtype=DataType.SPARSE_FLOAT_VECTOR,
            ),
            FieldSchema(name="source", dtype=DataType.VARCHAR, max_length=1024),
            FieldSchema(name="heading", dtype=DataType.VARCHAR, max_length=1024),
            FieldSchema(name="heading_level", dtype=DataType.INT64),
            FieldSchema(name="start_line", dtype=DataType.INT64),
            FieldSchema(name="end_line", dtype=DataType.INT64),
        ]

        bm25_function = Function(
            name="bm25_fn",
            function_type=FunctionType.BM25,
            input_field_names=["content"],
            output_field_names=["sparse_vector"],
        )

        schema = CollectionSchema(
            fields=fields, functions=[bm25_function],
            enable_dynamic_field=True,
        )

        index_params = self._client.prepare_index_params()
        index_params.add_index(
            field_name="embedding",
            index_type="FLAT",
            metric_type="COSINE",
        )
        index_params.add_index(
            field_name="sparse_vector",
            index_type="SPARSE_INVERTED_INDEX",
            metric_type="BM25",
        )

        self._client.create_collection(
            collection_name=self._collection,
            schema=schema,
            index_params=index_params,
        )

    def _has_sparse_field(self) -> bool:
        """Check if the collection has a sparse_vector field (hybrid-capable)."""
        try:
            info = self._client.describe_collection(self._collection)
            field_names = {f["name"] for f in info.get("fields", [])}
            return "sparse_vector" in field_names
        except Exception:
            return False

    def existing_hashes(self, hashes: list[str]) -> set[str]:
        """Return the subset of *hashes* that already exist in the collection."""
        if not hashes:
            return set()
        hash_list = ", ".join(f'"{h}"' for h in hashes)
        results = self._client.query(
            collection_name=self._collection,
            filter=f"chunk_hash in [{hash_list}]",
            output_fields=["chunk_hash"],
        )
        return {r["chunk_hash"] for r in results}

    def upsert(self, chunks: list[dict[str, Any]]) -> int:
        """Insert or update chunks (keyed by ``chunk_hash`` primary key).

        Note: ``sparse_vector`` is auto-generated by the BM25 Function
        from ``content`` â€” do NOT include it in chunk dicts.
        """
        if not chunks:
            return 0
        result = self._client.upsert(
            collection_name=self._collection,
            data=chunks,
        )
        return result.get("upsert_count", len(chunks)) if isinstance(result, dict) else len(chunks)

    def search(
        self,
        query_embedding: list[float],
        *,
        query_text: str = "",
        top_k: int = 10,
        filter_expr: str = "",
    ) -> list[dict[str, Any]]:
        """Hybrid search combining dense vector and BM25 full-text.

        If the collection supports BM25 (has sparse_vector field) and
        ``query_text`` is provided, performs hybrid search with RRF
        reranking.  Otherwise falls back to dense-only search.
        """
        use_hybrid = bool(query_text) and self._has_sparse_field()

        if use_hybrid:
            return self._hybrid_search(
                query_embedding, query_text,
                top_k=top_k, filter_expr=filter_expr,
            )
        return self._dense_search(
            query_embedding, top_k=top_k, filter_expr=filter_expr,
        )

    def _dense_search(
        self,
        query_embedding: list[float],
        *,
        top_k: int = 10,
        filter_expr: str = "",
    ) -> list[dict[str, Any]]:
        """Pure dense vector search (cosine similarity)."""
        kwargs: dict[str, Any] = {
            "collection_name": self._collection,
            "data": [query_embedding],
            "anns_field": "embedding",
            "limit": top_k,
            "output_fields": self._QUERY_FIELDS,
        }
        if filter_expr:
            kwargs["filter"] = filter_expr
        results = self._client.search(**kwargs)
        if not results or not results[0]:
            return []
        return [
            {**hit["entity"], "score": hit["distance"]}
            for hit in results[0]
        ]

    def _hybrid_search(
        self,
        query_embedding: list[float],
        query_text: str,
        *,
        top_k: int = 10,
        filter_expr: str = "",
    ) -> list[dict[str, Any]]:
        """Hybrid search: dense vector + BM25 full-text with RRF reranking."""
        from pymilvus import AnnSearchRequest, RRFRanker

        req_kwargs: dict[str, Any] = {}
        if filter_expr:
            req_kwargs["expr"] = filter_expr

        dense_req = AnnSearchRequest(
            data=[query_embedding],
            anns_field="embedding",
            param={"metric_type": "COSINE", "params": {}},
            limit=top_k,
            **req_kwargs,
        )

        bm25_req = AnnSearchRequest(
            data=[query_text],
            anns_field="sparse_vector",
            param={"metric_type": "BM25"},
            limit=top_k,
            **req_kwargs,
        )

        results = self._client.hybrid_search(
            collection_name=self._collection,
            reqs=[dense_req, bm25_req],
            ranker=RRFRanker(k=60),
            limit=top_k,
            output_fields=self._QUERY_FIELDS,
        )

        if not results or not results[0]:
            return []
        return [
            {**hit["entity"], "score": hit["distance"]}
            for hit in results[0]
        ]

    _QUERY_FIELDS = [
        "content", "source", "heading", "chunk_hash",
        "heading_level", "start_line", "end_line",
    ]

    def query(self, *, filter_expr: str = "") -> list[dict[str, Any]]:
        """Retrieve chunks by scalar filter (no vector needed)."""
        kwargs: dict[str, Any] = {
            "collection_name": self._collection,
            "output_fields": self._QUERY_FIELDS,
            "filter": filter_expr if filter_expr else 'chunk_hash != ""',
        }
        return self._client.query(**kwargs)

    def hashes_by_source(self, source: str) -> set[str]:
        """Return all chunk_hash values for a given source file."""
        results = self._client.query(
            collection_name=self._collection,
            filter=f'source == "{source}"',
            output_fields=["chunk_hash"],
        )
        return {r["chunk_hash"] for r in results}

    def indexed_sources(self) -> set[str]:
        """Return all distinct source values in the collection."""
        results = self._client.query(
            collection_name=self._collection,
            filter='chunk_hash != ""',
            output_fields=["source"],
        )
        return {r["source"] for r in results}

    def delete_by_source(self, source: str) -> None:
        """Delete all chunks from a given source file."""
        self._client.delete(
            collection_name=self._collection,
            filter=f'source == "{source}"',
        )

    def delete_by_hashes(self, hashes: list[str]) -> None:
        """Delete chunks by their content hashes (primary keys)."""
        if not hashes:
            return
        self._client.delete(
            collection_name=self._collection,
            ids=hashes,
        )

    def count(self) -> int:
        """Return total number of stored chunks."""
        stats = self._client.get_collection_stats(self._collection)
        return stats.get("row_count", 0)

    def drop(self) -> None:
        """Drop the entire collection."""
        if self._client.has_collection(self._collection):
            self._client.drop_collection(self._collection)

    def close(self) -> None:
        self._client.close()

    def __enter__(self) -> MilvusStore:
        return self

    def __exit__(self, *exc: object) -> None:
        self.close()
