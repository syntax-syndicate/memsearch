{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"memsearch","text":"<p>OpenClaw's memory, everywhere.</p> <p>Inspired by OpenClaw's memory system, memsearch brings the same markdown-first architecture to a standalone library. Pluggable into any agent framework, backed by Milvus.</p>"},{"location":"#why-memsearch","title":"Why memsearch?","text":"<pre><code>$ cat /dev/philosophy\nMarkdown is the source of truth.\nSimple. Readable. Git-friendly. Zero vendor lock-in.\nThe vector store is just a derived index \u2014 rebuildable anytime.\n</code></pre> <ul> <li>OpenClaw's memory, everywhere -- markdown as the single source of truth</li> <li>Smart dedup -- SHA-256 content hashing means unchanged content is never re-embedded</li> <li>Live sync -- file watcher auto-indexes on changes, deletes stale chunks</li> <li>Memory compact -- LLM-powered summarization compresses old memories</li> <li>Ready-made Claude Code plugin -- a drop-in example of agent memory built on memsearch</li> </ul>"},{"location":"#what-is-memsearch","title":"What is memsearch?","text":"<p>Most memory systems treat the vector database as the source of truth. memsearch flips this around: your markdown files are the source of truth, and the vector store is just a derived index -- like a database index that can be dropped and rebuilt at any time.</p> <p>This means:</p> <ul> <li>Your data is always human-readable -- plain <code>.md</code> files you can open, edit, grep, and <code>git diff</code></li> <li>No vendor lock-in -- switch embedding providers or vector backends without losing anything</li> <li>Rebuild on demand -- corrupted index? Just re-run <code>memsearch index</code> and you are back in seconds</li> <li>Git-native -- version your knowledge base with standard git workflows</li> </ul> <p>memsearch scans your markdown directories, splits content into semantically meaningful chunks (by heading structure and paragraph boundaries), embeds them, and stores the vectors in Milvus. When you search, it finds the most relevant chunks by cosine similarity and returns them with full source attribution.</p>"},{"location":"#quick-install","title":"Quick Install","text":"<pre><code>$ pip install memsearch\n</code></pre> <p>Say you have a directory of daily markdown logs (the same layout used by OpenClaw):</p> <pre><code>memory/\n\u251c\u2500\u2500 MEMORY.md          # persistent facts &amp; decisions\n\u251c\u2500\u2500 2026-02-07.md      # daily log\n\u251c\u2500\u2500 2026-02-08.md\n\u2514\u2500\u2500 2026-02-09.md\n</code></pre> <p>Index it and search:</p> <pre><code>$ memsearch index ./memory/\nIndexed 38 chunks.\n\n$ memsearch search \"how to configure Redis?\"\n\n--- Result 1 (score: 0.0328) ---\nSource: memory/2026-02-08.md\nHeading: Infrastructure Decisions\nWe chose Redis for caching over Memcached. Config: host=localhost,\nport=6379, max_memory=256mb, eviction=allkeys-lru.\n\n--- Result 2 (score: 0.0315) ---\nSource: memory/2026-02-07.md\nHeading: Redis Setup Notes\nRedis config for production: enable AOF persistence, set maxmemory-policy\nto volatile-lfu, bind to 127.0.0.1 only...\n\n$ memsearch watch ./memory/\nIndexed 8 chunks.\nWatching 1 path(s) for changes... (Ctrl+C to stop)\nIndexed 2 chunks from memory/2026-02-09.md\n</code></pre> <p>The <code>watch</code> command monitors your files and auto-indexes changes in the background -- ideal for use alongside editors or agent processes that write to your knowledge base.</p>"},{"location":"#python-api","title":"Python API","text":"<p>The core workflow is three lines: create a <code>MemSearch</code> instance, index your files, and search.</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\n\nasync def main():\n    mem = MemSearch(paths=[\"./memory/\"])\n\n    # Index all markdown files (skips unchanged content automatically)\n    await mem.index()\n\n    # Semantic search -- returns ranked results with source attribution\n    results = await mem.search(\"how to configure Redis?\", top_k=5)\n    for r in results:\n        print(f\"[{r['score']:.2f}] {r['source']} -- {r['content'][:80]}\")\n\n    mem.close()\n\nasyncio.run(main())\n</code></pre> <p>See Getting Started for a complete walkthrough with agent memory loops.</p>"},{"location":"#use-cases","title":"Use Cases","text":""},{"location":"#personal-knowledge-base","title":"Personal Knowledge Base","text":"<p>Point memsearch at your notes directory and get instant semantic search across years of accumulated knowledge.</p> <pre><code>$ memsearch index ~/notes/\n$ memsearch search \"that article about distributed consensus\"\n</code></pre>"},{"location":"#agent-memory","title":"Agent Memory","text":"<p>Give your AI agent persistent, searchable memory. The agent writes observations to markdown files; memsearch indexes them and retrieves relevant context on the next turn. This is exactly how OpenClaw manages memory, and memsearch ships with a ready-made Claude Code plugin that demonstrates the pattern.</p> <pre><code>mem = MemSearch(paths=[\"./agent-memory/\"])\n\n# Agent recalls relevant past experiences before responding\nmemories = await mem.search(user_question, top_k=3)\n\n# Agent saves new knowledge after responding\nsave_to_markdown(\"./agent-memory/\", today, summary)\nawait mem.index()\n</code></pre>"},{"location":"#team-knowledge-sharing","title":"Team Knowledge Sharing","text":"<p>Deploy a shared Milvus server and point multiple team members (or agents) at it. Everyone indexes their own markdown files into the same collection, creating a shared searchable knowledge base.</p> <pre><code>mem = MemSearch(\n    paths=[\"./docs/\"],\n    milvus_uri=\"http://milvus.internal:19530\",\n    milvus_token=\"root:Milvus\",\n)\n</code></pre>"},{"location":"#embedding-providers","title":"Embedding Providers","text":"<p>memsearch supports 5 embedding providers out of the box -- from cloud APIs to fully local models:</p> Provider Install OpenAI (default) <code>memsearch</code> (included) Google Gemini <code>memsearch[google]</code> Voyage AI <code>memsearch[voyage]</code> Ollama (local) <code>memsearch[ollama]</code> sentence-transformers (local) <code>memsearch[local]</code> <p>For fully local operation with no API keys, install <code>memsearch[ollama]</code> or <code>memsearch[local]</code>. See Getting Started for API key setup and provider details.</p>"},{"location":"#milvus-backend","title":"Milvus Backend","text":"<p>memsearch supports three deployment modes -- just change the URI:</p> Mode URI Use Case Milvus Lite (default) <code>~/.memsearch/milvus.db</code> Local file, zero config, single user Milvus Server <code>http://localhost:19530</code> Self-hosted, multi-agent, team use Zilliz Cloud <code>https://in03-xxx.zillizcloud.com</code> Fully managed, auto-scaling <p>See Getting Started for connection examples and Docker setup instructions.</p>"},{"location":"#configuration","title":"Configuration","text":"<p>memsearch uses a layered configuration system (lowest \u2192 highest priority):</p> <p>Built-in defaults \u2192 Global config (<code>~/.memsearch/config.toml</code>) \u2192 Project config (<code>.memsearch.toml</code>) \u2192 CLI flags</p> <pre><code>$ memsearch config init               # Interactive wizard\n$ memsearch config set milvus.uri http://localhost:19530\n$ memsearch config list --resolved    # Show merged config from all sources\n</code></pre> <p>API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>) are read from standard environment variables by their respective SDKs -- they are not stored in config files. See Getting Started for the full configuration guide.</p>"},{"location":"#license","title":"License","text":"<p>MIT</p>"},{"location":"architecture/","title":"Architecture","text":"<p>This page explains the architecture, design philosophy, and key implementation decisions behind memsearch.</p>"},{"location":"architecture/#design-philosophy","title":"Design Philosophy","text":""},{"location":"architecture/#markdown-as-the-source-of-truth","title":"Markdown as the Source of Truth","text":"<p>The foundational principle of memsearch is simple: markdown files are the canonical data store. The vector database is a derived index -- it can be dropped and rebuilt at any time from the markdown files on disk. This is the same philosophy used by OpenClaw's memory system, and memsearch is designed as a standalone library inspired by that architecture.</p> <p>Why markdown?</p> <ul> <li>Human-readable. Any developer can open a memory file in any text editor and understand what the agent knows. There is no binary format to decode, no special viewer required.</li> <li>Git-friendly. Markdown diffs are clean and meaningful. You get full version history, blame, branching, and merge conflict resolution for free -- the same tools you already use for code.</li> <li>Zero vendor lock-in. Markdown is a plain-text format that has been stable for decades. If you stop using memsearch tomorrow, your knowledge base is still right there on disk, fully intact.</li> <li>Trivially portable. Copy the files to another machine, another tool, another agent framework. No export step, no migration script, no schema translation.</li> </ul> <p>Why NOT a database as the source of truth?</p> <ul> <li>Opaque. Database files are binary blobs that require specific software to read. If the tool disappears, so does easy access to your data.</li> <li>Vendor lock-in. Each database engine has its own storage format, query language, and migration tooling. Switching costs are high.</li> <li>Fragile. Database corruption, version incompatibilities, and backup complexity are real operational concerns for what should be a simple knowledge store.</li> </ul> <p>In memsearch, the vector store is an acceleration layer -- nothing more. If the Milvus database is lost, corrupted, or simply out of date, a single <code>memsearch index</code> command rebuilds the entire index from the markdown files.</p> <pre><code>graph LR\n    MD[\"Markdown Files&lt;br&gt;(source of truth)\"] --&gt;|index| MIL[(Milvus&lt;br&gt;derived index)]\n    MIL --&gt;|lost or corrupted?| REBUILD[\"memsearch index&lt;br&gt;(full rebuild)\"]\n    REBUILD --&gt; MIL\n\n    style MD fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style MIL fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre>"},{"location":"architecture/#inspired-by-openclaw","title":"Inspired by OpenClaw","text":"<p>memsearch follows OpenClaw's memory architecture precisely:</p> Concept OpenClaw memsearch Memory layout <code>MEMORY.md</code> + <code>memory/YYYY-MM-DD.md</code> Same Chunk ID format <code>hash(source:startLine:endLine:contentHash:model)</code> Same Dedup strategy Content-hash primary key Same Compact target Append to daily markdown log Same Source of truth Markdown files (vector DB is derived) Same File watch debounce 1500ms Same default <p>If you are already using OpenClaw's memory directory layout, memsearch works with it directly -- no migration needed.</p>"},{"location":"architecture/#pipeline-overview","title":"Pipeline Overview","text":""},{"location":"architecture/#search-flow","title":"Search Flow","text":"<p>When a query arrives, it is embedded into a vector, then used for hybrid search (dense cosine similarity + BM25 full-text) against the Milvus collection. Results are reranked using Reciprocal Rank Fusion (RRF) and returned with source metadata.</p> <pre><code>graph LR\n    Q[/\"Query\"/] --&gt; E[Embed query] --&gt; HS[\"Hybrid Search&lt;br&gt;(Dense + BM25)\"]\n    HS --&gt; RRF[\"RRF Reranker&lt;br&gt;(k=60)\"] --&gt; R[Top-K Results]\n\n    subgraph Milvus\n        HS\n        RRF\n    end</code></pre>"},{"location":"architecture/#ingest-flow","title":"Ingest Flow","text":"<p>Markdown files are scanned, chunked by headings, and deduplicated using SHA-256 content hashes. Only new or changed chunks are sent to the embedding API and upserted into Milvus. Chunks from deleted files are automatically cleaned up.</p> <pre><code>graph LR\n    F[\"Markdown files\"] --&gt; SC[Scanner] --&gt; C[Chunker] --&gt; D{\"Dedup&lt;br&gt;(SHA-256)\"}\n    D --&gt;|new| E[Embed &amp; Upsert]\n    D --&gt;|exists| S[Skip]\n    D --&gt;|stale| DEL[Delete from Milvus]</code></pre>"},{"location":"architecture/#watch-and-compact","title":"Watch and Compact","text":"<p>The file watcher monitors directories for markdown changes and automatically re-indexes modified files. The compact operation compresses indexed chunks into an LLM-generated summary and writes it back to a daily markdown log -- which the watcher then picks up and indexes, closing the loop.</p> <pre><code>graph LR\n    W[File Watcher] --&gt;|1500ms debounce| I[Auto re-index]\n    FL[Compact] --&gt; L[LLM Summarize] --&gt; MD[\"memory/YYYY-MM-DD.md\"]\n    MD -.-&gt;|triggers| W</code></pre>"},{"location":"architecture/#chunking-strategy","title":"Chunking Strategy","text":"<p>memsearch splits markdown files into semantic chunks using a heading-based strategy, with paragraph-level fallback for oversized sections.</p>"},{"location":"architecture/#heading-based-chunking","title":"Heading-Based Chunking","text":"<p>The chunker treats markdown headings (<code>#</code> through <code>######</code>) as natural chunk boundaries. Each heading and the content below it (up to the next heading of equal or higher level) becomes one chunk. Content before the first heading (the \"preamble\") is treated as its own chunk.</p> <pre><code># Project Notes                    &lt;-- preamble chunk starts here\n\nSome introductory text.\n\n## Redis Configuration              &lt;-- chunk boundary\n\nWe chose Redis for caching...\n\n### Connection Settings              &lt;-- chunk boundary\n\nhost=localhost, port=6379...\n\n## Authentication                    &lt;-- chunk boundary\n\nWe use JWT tokens...\n</code></pre>"},{"location":"architecture/#paragraph-based-splitting-for-large-sections","title":"Paragraph-Based Splitting for Large Sections","text":"<p>When a heading-delimited section exceeds <code>max_chunk_size</code> (default: 1500 characters), the chunker splits it further at paragraph boundaries (blank lines). A configurable <code>overlap_lines</code> (default: 2 lines) is carried forward between sub-chunks to preserve context continuity.</p>"},{"location":"architecture/#chunk-metadata","title":"Chunk Metadata","text":"<p>Each chunk carries rich metadata for provenance tracking:</p> Field Description <code>content</code> The raw text of the chunk <code>source</code> Absolute file path the chunk was extracted from <code>heading</code> The nearest heading text (empty string for preamble) <code>heading_level</code> Heading depth: 1--6 for <code>#</code>--<code>######</code>, 0 for preamble <code>start_line</code> First line number in the source file (1-indexed) <code>end_line</code> Last line number in the source file <code>content_hash</code> Truncated SHA-256 hash of the chunk content (16 hex chars)"},{"location":"architecture/#deduplication","title":"Deduplication","text":"<p>memsearch uses content-addressable storage to avoid redundant embedding API calls and duplicate data in the vector store.</p>"},{"location":"architecture/#how-it-works","title":"How It Works","text":"<ol> <li>Each chunk's content is hashed with SHA-256 (truncated to 16 hex characters).</li> <li>A composite chunk ID is computed from the source path, line range, content hash, and embedding model name -- matching OpenClaw's format: <code>hash(markdown:source:startLine:endLine:contentHash:model)</code>.</li> <li>Before embedding, the set of existing chunk IDs for the source file is queried from Milvus.</li> <li>Only chunks whose composite ID is not already present get embedded and upserted.</li> <li>Chunks whose composite ID no longer appears in the re-chunked file are deleted (stale chunk cleanup).</li> </ol> <pre><code>graph TD\n    C[\"Chunk content\"] --&gt; H[\"SHA-256&lt;br&gt;(content_hash)\"]\n    H --&gt; CID[\"Composite ID&lt;br&gt;hash(source:lines:contentHash:model)\"]\n    CID --&gt; CHECK{\"Exists in&lt;br&gt;Milvus?\"}\n    CHECK --&gt;|No| EMBED[\"Embed &amp; Upsert\"]\n    CHECK --&gt;|Yes| SKIP[\"Skip&lt;br&gt;(save API cost)\"]</code></pre>"},{"location":"architecture/#why-this-matters","title":"Why This Matters","text":"<ul> <li>No external cache needed. The hash IS the primary key in Milvus. There is no SQLite sidecar database, no Redis cache, no <code>.json</code> tracking file. The deduplication mechanism is the storage key itself.</li> <li>Incremental indexing. Re-running <code>memsearch index</code> on an unchanged knowledge base produces zero embedding API calls. Only genuinely new or modified content is processed.</li> <li>Cost savings. Embedding API calls are the primary cost of running a semantic search system. Content-addressable dedup ensures you never pay to embed the same content twice.</li> </ul>"},{"location":"architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"architecture/#collection-schema","title":"Collection Schema","text":"<p>All chunks are stored in a single Milvus collection named <code>memsearch_chunks</code> (configurable). The schema uses both dense and sparse vector fields to enable hybrid search:</p> Field Type Purpose <code>chunk_hash</code> <code>VARCHAR(64)</code> Primary key -- composite SHA-256 chunk ID <code>embedding</code> <code>FLOAT_VECTOR</code> Dense embedding from the configured provider <code>content</code> <code>VARCHAR(65535)</code> Raw chunk text (also feeds BM25 via Milvus Function) <code>sparse_vector</code> <code>SPARSE_FLOAT_VECTOR</code> Auto-generated BM25 sparse vector <code>source</code> <code>VARCHAR(1024)</code> File path the chunk was extracted from <code>heading</code> <code>VARCHAR(1024)</code> Nearest heading text <code>heading_level</code> <code>INT64</code> Heading depth (0 = preamble) <code>start_line</code> <code>INT64</code> First line number in source file <code>end_line</code> <code>INT64</code> Last line number in source file <p>The <code>sparse_vector</code> field is populated automatically by a Milvus BM25 Function that processes the <code>content</code> field -- no application-side sparse encoding is needed.</p>"},{"location":"architecture/#hybrid-search","title":"Hybrid Search","text":"<p>Search combines two retrieval strategies and merges their results:</p> <ol> <li>Dense vector search -- cosine similarity on the <code>embedding</code> field (semantic meaning).</li> <li>BM25 sparse search -- keyword matching on the <code>sparse_vector</code> field (exact term overlap).</li> <li>RRF reranking -- Reciprocal Rank Fusion with k=60 merges the two ranked lists into a single result set.</li> </ol> <p>This hybrid approach catches results that pure semantic search might miss (exact names, error codes, configuration values) while still benefiting from the semantic understanding that dense embeddings provide.</p>"},{"location":"architecture/#three-tier-deployment","title":"Three-Tier Deployment","text":"<p>memsearch supports three Milvus deployment modes. Switch between them by changing a single parameter (<code>milvus_uri</code>):</p> <pre><code>graph TD\n    A[\"memsearch\"] --&gt; B{\"milvus_uri\"}\n    B --&gt;|\"~/.memsearch/milvus.db&lt;br&gt;(default)\"| C[\"Milvus Lite&lt;br&gt;Local .db file&lt;br&gt;Zero config\"]\n    B --&gt;|\"http://host:19530\"| D[\"Milvus Server&lt;br&gt;Self-hosted&lt;br&gt;Docker / K8s\"]\n    B --&gt;|\"https://...zillizcloud.com\"| E[\"Zilliz Cloud&lt;br&gt;Fully managed&lt;br&gt;Auto-scaling\"]\n\n    style C fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style D fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style E fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1</code></pre> Tier URI Pattern Use Case Milvus Lite <code>~/.memsearch/milvus.db</code> Personal use, single agent, development. No server to install. Milvus Server <code>http://localhost:19530</code> Multi-agent teams, shared infrastructure, CI/CD. Deploy via Docker or Kubernetes. Zilliz Cloud <code>https://...zillizcloud.com</code> Production SaaS, zero-ops, auto-scaling. Free tier available at cloud.zilliz.com."},{"location":"architecture/#physical-isolation","title":"Physical Isolation","text":"<p>All agents and projects share the same collection name (<code>memsearch_chunks</code>) by default. Physical isolation between agents is achieved by pointing each one to a different <code>milvus_uri</code> -- each agent gets its own Milvus Lite database file, its own Milvus server, or its own Zilliz Cloud cluster. This avoids the complexity of multi-tenant collection management while keeping the schema simple.</p>"},{"location":"architecture/#configuration-system","title":"Configuration System","text":"<p>memsearch uses a 4-layer configuration system. Each layer overrides the one before it:</p> <pre><code>graph LR\n    D[\"1. Defaults\"] --&gt; G[\"2. Global Config&lt;br&gt;~/.memsearch/config.toml\"]\n    G --&gt; P[\"3. Project Config&lt;br&gt;.memsearch.toml\"]\n    P --&gt; C[\"4. CLI Flags&lt;br&gt;--milvus-uri, etc.\"]</code></pre> Priority Source Scope Example 1 (lowest) Built-in defaults Hardcoded <code>milvus.uri = ~/.memsearch/milvus.db</code> 2 <code>~/.memsearch/config.toml</code> User-global Shared across all projects 3 <code>.memsearch.toml</code> Per-project Committed to the repo or gitignored 4 (highest) CLI flags Per-command <code>--milvus-uri http://...</code> <p>Note: API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>) are read from environment variables by their respective SDKs. They are not part of the memsearch configuration system and are never written to config files.</p>"},{"location":"architecture/#config-sections","title":"Config Sections","text":"<p>The full configuration is organized into five sections:</p> <pre><code>[milvus]\nuri = \"~/.memsearch/milvus.db\"\ntoken = \"\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"                           # empty = provider default\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"                       # empty = provider default\nprompt_file = \"\"                     # custom prompt template path\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n</code></pre>"},{"location":"architecture/#data-flow-overview","title":"Data Flow Overview","text":"<p>The following diagram shows the complete data flow from source-of-truth markdown files through processing and into the derived vector store:</p> <pre><code>graph TB\n    subgraph \"Source of Truth\"\n        MEM[\"MEMORY.md\"]\n        D1[\"memory/2026-02-08.md\"]\n        D2[\"memory/2026-02-09.md\"]\n    end\n\n    subgraph \"Processing\"\n        SCAN[Scanner] --&gt; CHUNK[Chunker]\n        CHUNK --&gt; HASH[\"SHA-256&lt;br&gt;Dedup\"]\n    end\n\n    subgraph \"Storage (derived)\"\n        EMB[Embedding API] --&gt; MIL[(Milvus)]\n    end\n\n    MEM &amp; D1 &amp; D2 --&gt; SCAN\n    HASH --&gt;|new chunks| EMB\n    MIL --&gt;|search| RES[Results]\n\n    style MEM fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style D1 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style D2 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style MIL fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre>"},{"location":"architecture/#the-compact-cycle","title":"The Compact Cycle","text":"<p>The compact operation creates a feedback loop that keeps the knowledge base compact:</p> <pre><code>graph LR\n    CHUNKS[\"Indexed chunks&lt;br&gt;in Milvus\"] --&gt; RETRIEVE[\"Retrieve all&lt;br&gt;(or filtered)\"]\n    RETRIEVE --&gt; LLM[\"LLM Summarize&lt;br&gt;(OpenAI / Anthropic / Gemini)\"]\n    LLM --&gt; WRITE[\"Append to&lt;br&gt;memory/YYYY-MM-DD.md\"]\n    WRITE --&gt; WATCH[\"File watcher&lt;br&gt;detects change\"]\n    WATCH --&gt; REINDEX[\"Auto re-index&lt;br&gt;updated file\"]\n    REINDEX --&gt; CHUNKS\n\n    style WRITE fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style CHUNKS fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1</code></pre> <ol> <li>All (or filtered) chunks are retrieved from Milvus.</li> <li>An LLM compresses them into a concise summary preserving key facts, decisions, and code patterns.</li> <li>The summary is appended to a daily markdown log (<code>memory/YYYY-MM-DD.md</code>).</li> <li>The file watcher detects the change and re-indexes the updated file.</li> <li>The cycle completes: the compressed knowledge is now searchable, and the source-of-truth markdown has the full history.</li> </ol>"},{"location":"architecture/#security","title":"Security","text":""},{"location":"architecture/#local-first-by-default","title":"Local-First by Default","text":"<p>The entire memsearch pipeline runs locally by default:</p> <ul> <li>Milvus Lite stores data in a local <code>.db</code> file on your filesystem.</li> <li>Local embedding providers (<code>memsearch[local]</code> with sentence-transformers, or <code>memsearch[ollama]</code> with a local Ollama server) process text without any network calls.</li> </ul> <p>In a fully local configuration, your data never leaves your machine.</p>"},{"location":"architecture/#when-data-leaves-your-machine","title":"When Data Leaves Your Machine","text":"<p>Data is transmitted externally only when you explicitly choose a remote component:</p> Component Local Option Remote Option Vector store Milvus Lite (default) Milvus Server, Zilliz Cloud Embeddings <code>local</code>, <code>ollama</code> <code>openai</code>, <code>google</code>, <code>voyage</code> Compact LLM Ollama (local) OpenAI, Anthropic, Gemini"},{"location":"architecture/#api-key-handling","title":"API Key Handling","text":"<p>API keys are read from standard environment variables (<code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>, <code>VOYAGE_API_KEY</code>, <code>ANTHROPIC_API_KEY</code>). They are never written to config files by memsearch, never logged, and never stored in the vector database.</p>"},{"location":"architecture/#filesystem-access","title":"Filesystem Access","text":"<p>memsearch reads only the directories and files you explicitly configure via <code>paths</code>. It does not scan outside those paths. Hidden files and directories (those starting with <code>.</code>) are skipped by default during scanning.</p>"},{"location":"claude-plugin/","title":"Claude Code Plugin","text":"<p>Automatic persistent memory for Claude Code. No commands to learn, no manual saving -- just install the plugin and Claude remembers what you worked on across sessions.</p> <p>The plugin is built entirely on Claude Code's own primitives: Hooks for lifecycle events, CLI for tool access, and Agent for autonomous decisions. No MCP servers, no sidecar services, no extra network round-trips. Everything runs locally as shell scripts and a Python CLI.</p>"},{"location":"claude-plugin/#what-it-does","title":"What It Does","text":"<p>When you launch Claude Code with the memsearch plugin:</p> <ol> <li>Every session is remembered. When Claude finishes responding, a Haiku model summarizes the exchange and appends it to a daily markdown log (<code>YYYY-MM-DD.md</code>).</li> <li>Every prompt triggers recall. Before Claude sees your message, a semantic search runs against all past memories and injects the most relevant ones into context.</li> <li>No manual intervention. You never need to run a command, tag a memory, or tell Claude to \"remember this\". The hooks handle everything.</li> </ol> <p>The result: Claude has a persistent, searchable, ever-growing memory -- without you lifting a finger.</p>"},{"location":"claude-plugin/#quick-start","title":"Quick Start","text":""},{"location":"claude-plugin/#install-from-marketplace-recommended","title":"Install from Marketplace (recommended)","text":"<pre><code># 1. Install the memsearch CLI\npip install memsearch\n\n# 2. (Optional) Initialize config\nmemsearch config init\n\n# 3. In Claude Code, add the marketplace and install the plugin\n/plugin marketplace add zilliztech/memsearch\n/plugin install memsearch\n\n# 4. Have a conversation, then exit. Check your memories:\ncat .memsearch/memory/$(date +%Y-%m-%d).md\n\n# 5. Start a new session -- Claude automatically remembers!\n</code></pre>"},{"location":"claude-plugin/#development-mode","title":"Development mode","text":"<p>For contributors or if you want to modify the plugin:</p> <pre><code>git clone https://github.com/zilliztech/memsearch.git\npip install memsearch\nclaude --plugin-dir ./memsearch/ccplugin\n</code></pre>"},{"location":"claude-plugin/#how-it-works","title":"How It Works","text":"<p>The plugin hooks into 4 Claude Code lifecycle events. A singleton <code>memsearch watch</code> process runs in the background, keeping the vector index in sync with markdown files as they change.</p>"},{"location":"claude-plugin/#lifecycle-diagram","title":"Lifecycle Diagram","text":"<pre><code>stateDiagram-v2\n    [*] --&gt; SessionStart\n    SessionStart --&gt; WatchRunning: start memsearch watch\n    SessionStart --&gt; InjectRecent: load recent memories\n\n    state WatchRunning {\n        [*] --&gt; Watching\n        Watching --&gt; Reindex: file changed\n        Reindex --&gt; Watching: done\n    }\n\n    InjectRecent --&gt; Prompting\n\n    state Prompting {\n        [*] --&gt; UserInput\n        UserInput --&gt; SemanticSearch: memsearch search\n        SemanticSearch --&gt; InjectMemories: top-k results\n        InjectMemories --&gt; ClaudeThinks\n        ClaudeThinks --&gt; Summary: haiku summarize\n        Summary --&gt; WriteMD: write YYYY-MM-DD.md\n        WriteMD --&gt; UserInput: next turn\n    }\n\n    Prompting --&gt; SessionEnd: user exits\n    SessionEnd --&gt; StopWatch: stop memsearch watch\n    StopWatch --&gt; [*]</code></pre>"},{"location":"claude-plugin/#hook-summary","title":"Hook Summary","text":"<p>The plugin defines exactly 4 hooks, all declared in <code>hooks/hooks.json</code>:</p> Hook Type Async Timeout What It Does SessionStart command no 10s Start <code>memsearch watch</code> singleton, write session heading to today's <code>.md</code>, inject recent memories and Memory Tools instructions via <code>additionalContext</code> UserPromptSubmit command no 15s Semantic search on user prompt (skip if &lt; 10 chars), inject top-3 relevant memories with <code>chunk_hash</code> IDs via <code>additionalContext</code> Stop command yes 120s Parse transcript with <code>parse-transcript.sh</code>, call <code>claude -p --model haiku</code> to summarize, append summary with session/turn anchors to daily <code>.md</code> SessionEnd command no 10s Stop the <code>memsearch watch</code> background process (cleanup)"},{"location":"claude-plugin/#what-each-hook-does","title":"What Each Hook Does","text":""},{"location":"claude-plugin/#sessionstart","title":"SessionStart","text":"<p>Fires once when a Claude Code session begins. This hook:</p> <ol> <li>Starts the watcher. Launches <code>memsearch watch .memsearch/memory/</code> as a singleton background process (PID file lock prevents duplicates). The watcher monitors markdown files and auto-re-indexes on changes with a 1500ms debounce.</li> <li>Writes a session heading. Appends <code>## Session HH:MM</code> to today's memory file (<code>.memsearch/memory/YYYY-MM-DD.md</code>), creating the file if it does not exist.</li> <li>Injects recent memories. Reads the last 30 lines from the 2 most recent daily logs. If memsearch is available, also runs <code>memsearch search \"recent session summary\" --top-k 3</code> for semantic results.</li> <li>Injects Memory Tools instructions. Tells Claude about <code>memsearch expand</code> and <code>memsearch transcript</code> commands for progressive disclosure (L2 and L3).</li> </ol> <p>All of this is returned as <code>additionalContext</code> in the hook output JSON.</p>"},{"location":"claude-plugin/#userpromptsubmit","title":"UserPromptSubmit","text":"<p>Fires on every user prompt before Claude processes it. This hook:</p> <ol> <li>Extracts the prompt from the hook input JSON.</li> <li>Skips short prompts (under 10 characters) -- greetings and single words are not worth searching.</li> <li>Runs semantic search. Calls <code>memsearch search \"$PROMPT\" --top-k 3 --json-output</code>.</li> <li>Formats results as a compact index with source file, heading, a 200-character preview, and the <code>chunk_hash</code> for each result.</li> <li>Injects as context. Returns formatted results under a <code>## Relevant Memories</code> heading via <code>additionalContext</code>.</li> </ol> <p>This is the key mechanism that makes memory recall automatic -- Claude does not need to decide to search, it simply receives relevant context on every prompt.</p>"},{"location":"claude-plugin/#stop","title":"Stop","text":"<p>Fires after Claude finishes each response. Runs asynchronously so it does not block the user. This hook:</p> <ol> <li>Guards against recursion. Checks <code>stop_hook_active</code> to prevent infinite loops (since the hook itself calls <code>claude -p</code>).</li> <li>Validates the transcript. Skips if the transcript file is missing or has fewer than 3 lines.</li> <li>Parses the transcript. Calls <code>parse-transcript.sh</code>, which:<ul> <li>Takes the last 200 lines of the JSONL transcript</li> <li>Truncates user/assistant text to 500 characters each</li> <li>Extracts tool names with input summaries</li> <li>Skips <code>file-history-snapshot</code> entries</li> </ul> </li> <li>Summarizes with Haiku. Pipes the parsed transcript to <code>claude -p --model haiku --no-session-persistence</code> with a system prompt that requests 3-8 bullet points focusing on decisions, problems solved, code changes, and key findings.</li> <li>Appends to daily log. Writes a <code>### HH:MM</code> sub-heading with an HTML comment anchor containing session ID, turn UUID, and transcript path. Then explicitly runs <code>memsearch index</code> to ensure the new content is indexed immediately, rather than relying on the watcher's debounce timer (which may not fire before SessionEnd kills the watcher).</li> </ol>"},{"location":"claude-plugin/#sessionend","title":"SessionEnd","text":"<p>Fires when the user exits Claude Code. Simply calls <code>stop_watch</code> to kill the <code>memsearch watch</code> process and clean up the PID file, including a sweep for any orphaned processes.</p>"},{"location":"claude-plugin/#progressive-disclosure","title":"Progressive Disclosure","text":"<p>Memory retrieval uses a three-layer progressive disclosure model. Layer 1 is fully automatic; layers 2 and 3 are available on demand when Claude needs more context.</p> <pre><code>graph TD\n    L1[\"L1: Auto-injected&lt;br/&gt;(UserPromptSubmit hook)\"] --&gt; L2[\"L2: On-demand expand&lt;br/&gt;(memsearch expand)\"]\n    L2 --&gt; L3[\"L3: Transcript drill-down&lt;br/&gt;(memsearch transcript)\"]\n\n    style L1 fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style L2 fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1\n    style L3 fill:#2a3a5c,stroke:#d66b6b,color:#a8b2c1</code></pre>"},{"location":"claude-plugin/#l1-auto-injected-automatic","title":"L1: Auto-Injected (Automatic)","text":"<p>On every user prompt, the <code>UserPromptSubmit</code> hook injects the top-3 semantic search results. Each result includes:</p> <ul> <li>Source file and heading</li> <li>A 200-character content preview</li> <li>The <code>chunk_hash</code> identifier</li> </ul> <p>This happens transparently -- no action from Claude or the user is required.</p> <p>Example injection (this is what Claude sees before processing each message):</p> <pre><code>## Relevant Memories\n- [.memsearch/memory/2026-02-12.md:04:16]  \u4f7f\u7528\u8fdc\u7a0b Milvus Server \u6d4b\u8bd5\n  `claude -p` \u6a21\u5f0f\uff0c\u7d22\u5f15\u4e86 4 \u4e2a chunks\uff0csearch \u6b63\u5e38\u8fd4\u56de\u76f8\u5173\u7ed3\u679c\u3002\u786e\u8ba4\n  `claude -p` \u6a21\u5f0f\u4e0d\u89e6\u53d1\u4efb\u4f55 hooks...\n  `chunk_hash: 47b5475122b992b6`\n- [.memsearch/memory/2026-02-11.md:11:02]  \u4fee\u590d watch \u542f\u52a8\u65f6\u4e0d\u7d22\u5f15\u5df2\u6709\u6587\u4ef6\u7684\n  \u95ee\u9898\u3002ccplugin stop.sh \u589e\u52a0\u663e\u5f0f memsearch index \u8c03\u7528...\n  `chunk_hash: 31cbaf74856ad1ed`\n</code></pre> <p>The preview is enough for Claude to answer most follow-up questions. But when it needs the full picture, it moves to L2.</p>"},{"location":"claude-plugin/#l2-on-demand-expand","title":"L2: On-Demand Expand","text":"<p>When an L1 preview is not enough, Claude runs <code>memsearch expand</code> to retrieve the full markdown section surrounding a chunk:</p> <pre><code>$ memsearch expand 47b5475122b992b6\n</code></pre> <p>Example output:</p> <pre><code>Source: .memsearch/memory/2026-02-12.md (lines 96-111)\nHeading: 04:16\nSession: 433f8bc3-a5a8-46a2-8285-71941dc96ad0\nTurn: 8ee6995b-2e7c-4e11-92e2-6f07fdfb55c7\nTranscript: /home/user/.claude/projects/.../433f8bc3...46a0.jsonl\n\n### 04:13\n&lt;!-- session:433f8bc3... turn:0a0df619... transcript:/.../433f8bc3...46a0.jsonl --&gt;\n- `claude -p` \u6a21\u5f0f\u4e0d\u89e6\u53d1 SessionStart \u548c UserPromptSubmit hooks\n- hooks \u4f9d\u8d56\u6b63\u5e38\u4ea4\u4e92\u6a21\u5f0f\n- Milvus Server \u53ef\u7528\uff0c\u8fde\u63a5\u5230 http://10.100.30.11:19530 \u9a8c\u8bc1\u901a\u8fc7\n- \u672c\u5730 Milvus Lite \u6d4b\u8bd5\u5b8c\u6210\uff0cindex \u4e86 4 \u4e2a chunks\n\n### 04:16\n&lt;!-- session:433f8bc3... turn:8ee6995b... transcript:/.../433f8bc3...46a0.jsonl --&gt;\n- \u4f7f\u7528\u8fdc\u7a0b Milvus Server \u6d4b\u8bd5 `claude -p` \u6a21\u5f0f\uff0c\u7d22\u5f15\u4e86 4 \u4e2a chunks\n- \u786e\u8ba4 `claude -p` \u6a21\u5f0f\u4e0d\u89e6\u53d1\u4efb\u4f55 hooks\n- \u6b63\u786e\u7684\u6d4b\u8bd5\u65b9\u5f0f\u5e94\u8be5\u7528\u4ea4\u4e92\u6a21\u5f0f `claude` \u800c\u975e `-p` \u6807\u5fd7\n- \u8fdc\u7a0b Milvus Server \u53ef\u7528\uff0ccollection stats \u663e\u793a 0\uff08\u5df2\u77e5\u7684 flush \u5ef6\u8fdf\u95ee\u9898\uff09\n</code></pre> <p>Now Claude sees the full context including the neighboring <code>### 04:13</code> section. The embedded <code>&lt;!-- session:... --&gt;</code> anchors link to the original conversation -- if Claude needs to go even deeper, it moves to L3.</p> <p>Additional flags:</p> <pre><code># JSON output with anchor metadata (for programmatic L3 drill-down)\nmemsearch expand 47b5475122b992b6 --json-output\n\n# Show N lines of context before/after instead of the full section\nmemsearch expand 47b5475122b992b6 --lines 10\n</code></pre>"},{"location":"claude-plugin/#l3-transcript-drill-down","title":"L3: Transcript Drill-Down","text":"<p>When Claude needs the original conversation verbatim -- for instance, to recall exact code snippets, error messages, or tool outputs -- it drills into the JSONL transcript.</p> <p>List all turns in a session:</p> <pre><code>$ memsearch transcript /path/to/session.jsonl\n</code></pre> <pre><code>All turns (73):\n\n  6d6210b7-b84  15:15:14  Implement the following plan: ...              [20 tools]\n  3075ee94-0f6  15:20:10  \u8fd9\u4e2accplugin\u7684\u4f8b\u5b50\u8fd8\u8981\u8bb2\u8981\u51c6\u5907 API key...\n  8e45ce0d-9a0  15:23:16  /plugin install memsearch \u540e\u9762\u8981\u6ce8\u91ca\u4e0b...       [2 tools]\n  53f5cac3-6d9  15:27:07  claude-mem \u94fe\u63a5\u597d\u50cf\u6253\u4e0d\u5f00...                    [9 tools]\n  c708b40c-8f8  15:30:45  \u8fd9\u4e9b\u6539\u52a8\u63d0\u4ea4\u4e0bpush\u7136\u540e\u63d0\u4e2apr...                [10 tools]\n</code></pre> <p>Each line shows the turn UUID prefix, timestamp, content preview, and how many tool calls occurred.</p> <p>Drill into a specific turn with surrounding context:</p> <pre><code>$ memsearch transcript /path/to/session.jsonl --turn 6d6210b7 --context 1\n</code></pre> <pre><code>Showing 2 turns around 6d6210b7:\n\n&gt;&gt;&gt; [15:15:14] 6d6210b7\nImplement the following plan:\n\n# Plan: Slim down README, link to docs site\n\n## Context\nREADME \u4e2d CLI Usage\u3001Configuration\u3001Embedding Providers \u7b49\u7ae0\u8282\u4e0e\u6587\u6863\u7ad9\n\u5185\u5bb9\u9ad8\u5ea6\u91cd\u590d\u3002\u7cbe\u7b80 README \u4fdd\u7559\u6838\u5fc3\u4eae\u70b9\uff0c\u8be6\u7ec6\u5185\u5bb9\u94fe\u63a5\u5230\u6587\u6863\u7ad9\u3002\n...\n\n**Assistant**: \u73b0\u5728\u6211\u6765\u770b\u770b\u6587\u6863\u7ad9\u7684\u951a\u70b9\uff0c\u786e\u4fdd\u94fe\u63a5\u6b63\u786e\u3002\n</code></pre> <p>This recovers the full original conversation -- user messages, assistant responses, and tool call summaries -- so Claude can recall exactly what happened during a past session.</p> <pre><code># JSON output for programmatic use\nmemsearch transcript /path/to/session.jsonl --turn 6d6210b7 --json-output\n</code></pre>"},{"location":"claude-plugin/#what-the-jsonl-looks-like","title":"What the JSONL Looks Like","text":"<p>The transcript files are standard JSON Lines -- one JSON object per line. Claude Code writes every message, tool call, and tool result as a separate line. Here is what the key message types look like (abbreviated for readability):</p> <p>User message (human input):</p> <pre><code>{\n  \"type\": \"user\",\n  \"uuid\": \"6d6210b7-b841-4cd7-a97f-e3c8bb185d06\",\n  \"parentUuid\": \"8404eaca-3926-4765-bcb9-6ca4befae466\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:14.284Z\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": \"Implement the following plan: ...\"\n  }\n}\n</code></pre> <p>Assistant message (text response):</p> <pre><code>{\n  \"type\": \"assistant\",\n  \"uuid\": \"32da9357-1efe-4985-8a6e-4864bbf58951\",\n  \"parentUuid\": \"d99f255c-6ac7-43fa-bcc8-c0dabc4c65cf\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:36.510Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\"type\": \"text\", \"text\": \"\u597d\u7684\uff0c\u8ba9\u6211\u5f00\u59cb\u7f16\u8f91 README.md\u3002\"}\n    ]\n  }\n}\n</code></pre> <p>Assistant message (tool call):</p> <pre><code>{\n  \"type\": \"assistant\",\n  \"uuid\": \"35fa9333-02ff-4b07-9036-ec0e3e290602\",\n  \"parentUuid\": \"7ab167db-9a57-4f51-b5d3-eb63a2e6a5ad\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:20.992Z\",\n  \"message\": {\n    \"role\": \"assistant\",\n    \"content\": [\n      {\n        \"type\": \"tool_use\",\n        \"id\": \"toolu_014CPfherKZMyYbbG5VT4dyX\",\n        \"name\": \"Read\",\n        \"input\": {\"file_path\": \"/path/to/README.md\"}\n      }\n    ]\n  }\n}\n</code></pre> <p>Tool result (returned to assistant as a user message):</p> <pre><code>{\n  \"type\": \"user\",\n  \"uuid\": \"7dd5ac66-c848-4e39-952a-511c94ac66f2\",\n  \"parentUuid\": \"35fa9333-02ff-4b07-9036-ec0e3e290602\",\n  \"sessionId\": \"433f8bc3-a5a8-46a2-8285-71941dc96ad0\",\n  \"timestamp\": \"2026-02-11T15:15:21.005Z\",\n  \"message\": {\n    \"role\": \"user\",\n    \"content\": [\n      {\n        \"type\": \"tool_result\",\n        \"tool_use_id\": \"toolu_014CPfherKZMyYbbG5VT4dyX\",\n        \"content\": \"     1\u2192# memsearch\\n     2\u2192\\n     3\u2192...\"\n      }\n    ]\n  }\n}\n</code></pre> <p>Key fields:</p> Field Description <code>type</code> Message type: <code>user</code>, <code>assistant</code>, <code>progress</code>, <code>system</code>, <code>file-history-snapshot</code> <code>uuid</code> Unique ID for this message <code>parentUuid</code> ID of the previous message (forms a linked chain) <code>sessionId</code> Session ID (matches the JSONL filename) <code>timestamp</code> ISO 8601 timestamp <code>message.content</code> String for user text, or array of <code>text</code> / <code>tool_use</code> / <code>tool_result</code> blocks <p>You don't need to parse JSONL manually</p> <p>The <code>memsearch transcript</code> command handles all the parsing, truncation, and formatting. The JSONL structure is documented here for transparency -- most users will never need to read these files directly.</p>"},{"location":"claude-plugin/#session-anchors","title":"Session Anchors","text":"<p>Each memory summary includes an HTML comment anchor that links the chunk back to its source session, enabling the L2-to-L3 drill-down:</p> <pre><code>### 14:30\n&lt;!-- session:abc123def turn:ghi789jkl transcript:/home/user/.claude/projects/.../abc123def.jsonl --&gt;\n- Implemented caching system with Redis L1 and in-process LRU L2\n- Fixed N+1 query issue in order-service using selectinload\n- Decided to use Prometheus counters for cache hit/miss metrics\n</code></pre> <p>The anchor contains three fields:</p> Field Description <code>session</code> Claude Code session ID (also the JSONL filename without extension) <code>turn</code> UUID of the last user turn in the session <code>transcript</code> Absolute path to the JSONL transcript file <p>Claude extracts these fields from <code>memsearch expand --json-output</code> and uses them to call <code>memsearch transcript</code> for L3 access.</p>"},{"location":"claude-plugin/#memory-storage","title":"Memory Storage","text":"<p>All memories live in <code>.memsearch/memory/</code> inside your project directory.</p>"},{"location":"claude-plugin/#directory-structure","title":"Directory Structure","text":"<pre><code>your-project/\n\u251c\u2500\u2500 .memsearch/\n\u2502   \u251c\u2500\u2500 .watch.pid            &lt;-- singleton watcher PID file\n\u2502   \u2514\u2500\u2500 memory/\n\u2502       \u251c\u2500\u2500 2026-02-07.md     &lt;-- daily memory log\n\u2502       \u251c\u2500\u2500 2026-02-08.md\n\u2502       \u2514\u2500\u2500 2026-02-09.md     &lt;-- today's session summaries\n\u2514\u2500\u2500 ... (your project files)\n</code></pre>"},{"location":"claude-plugin/#example-memory-file","title":"Example Memory File","text":"<p>A typical daily memory file (<code>2026-02-09.md</code>) looks like this:</p> <pre><code>## Session 14:30\n\n### 14:30\n&lt;!-- session:abc123def turn:ghi789jkl transcript:/home/user/.claude/projects/.../abc123def.jsonl --&gt;\n- Implemented caching system with Redis L1 and in-process LRU L2\n- Fixed N+1 query issue in order-service using selectinload\n- Decided to use Prometheus counters for cache hit/miss metrics\n\n## Session 17:45\n\n### 17:45\n&lt;!-- session:mno456pqr turn:stu012vwx transcript:/home/user/.claude/projects/.../mno456pqr.jsonl --&gt;\n- Debugged React hydration mismatch caused by Date.now() during SSR\n- Added comprehensive test suite for the caching middleware\n- Reviewed PR #42: approved with minor naming suggestions\n</code></pre> <p>Each file accumulates all sessions from that day. The format is plain markdown -- human-readable, <code>grep</code>-able, and git-friendly.</p>"},{"location":"claude-plugin/#markdown-is-the-source-of-truth","title":"Markdown Is the Source of Truth","text":"<p>The Milvus vector index is a derived cache that can be rebuilt at any time:</p> <pre><code>memsearch index .memsearch/memory/\n</code></pre> <p>This means:</p> <ul> <li>No data loss. Even if Milvus is corrupted or deleted, your memories are safe in <code>.md</code> files.</li> <li>Portable. Copy <code>.memsearch/memory/</code> to another machine and rebuild the index.</li> <li>Auditable. You can read, edit, or delete any memory entry with a text editor.</li> <li>Git-friendly. Commit your memory files to version control for a complete project history.</li> </ul>"},{"location":"claude-plugin/#comparison-with-claude-mem","title":"Comparison with claude-mem","text":"<p>claude-mem is another memory solution for Claude Code. Here is a detailed comparison:</p> Aspect memsearch claude-mem Architecture 4 shell hooks + 1 watch process Node.js/Bun worker service + Express server + React UI Integration Native hooks + CLI (zero IPC overhead) MCP server (stdio); tool definitions permanently consume context window Memory recall Automatic -- semantic search on every prompt via hook Agent-driven -- Claude must explicitly call MCP <code>search</code> tool Progressive disclosure 3-layer, auto-triggered: hook injects top-k (L1), then <code>expand</code> (L2), then <code>transcript</code> (L3) 3-layer, all manual: <code>search</code>, <code>timeline</code>, <code>get_observations</code> all require explicit tool calls Session summary cost 1 <code>claude -p --model haiku</code> call, runs async Observation on every tool use + session summary (more API calls at scale) Vector backend Milvus -- hybrid search (dense + BM25), scales from embedded to distributed cluster Chroma -- dense only, limited scaling path Storage format Transparent <code>.md</code> files -- human-readable, git-friendly Opaque SQLite + Chroma binary Index sync <code>memsearch watch</code> singleton -- auto-debounced background sync Automatic observation writes, but no unified background sync Data portability Copy <code>.memsearch/memory/*.md</code> and rebuild Export from SQLite + Chroma Runtime dependency Python (<code>memsearch</code> CLI) + <code>claude</code> CLI Node.js + Bun + MCP runtime Context window cost Minimal -- hook injects only top-k results as plain text MCP tool definitions always loaded + each tool call/result consumes context Cost per session ~1 Haiku call for summary Multiple Claude API calls for observation compression"},{"location":"claude-plugin/#the-key-insight-automatic-vs-agent-driven-recall","title":"The Key Insight: Automatic vs. Agent-Driven Recall","text":"<p>The fundamental architectural difference is when memory recall happens.</p> <p>memsearch injects relevant memories into every prompt via hooks. Claude does not need to decide whether to search -- it simply receives relevant context before processing each message. This means memories are never missed due to Claude forgetting to look them up. Progressive disclosure starts automatically at L1 (the hook injects top-k results), and only deeper layers (L2 expand, L3 transcript) require explicit CLI calls from the agent.</p> <p>claude-mem gives Claude MCP tools to search, explore timelines, and fetch observations. All three layers require Claude to proactively decide to invoke them. While this is more flexible (Claude controls when and what to recall), it means memories are only retrieved when Claude thinks to ask. In practice, Claude often does not call the search tool unless the conversation explicitly references past work -- which means relevant context can be silently lost.</p> <p>The difference is analogous to push vs. pull: memsearch pushes memories to Claude on every turn, while claude-mem requires Claude to pull them on demand.</p>"},{"location":"claude-plugin/#comparison-with-claudes-native-memory","title":"Comparison with Claude's Native Memory","text":"<p>Claude Code has built-in memory features: <code>CLAUDE.md</code> files and auto-memory (the <code>/memory</code> command). Here is why memsearch provides a stronger solution:</p> Aspect Claude Native Memory memsearch Storage Single <code>CLAUDE.md</code> file (or per-project) Unlimited daily <code>.md</code> files with full history Recall mechanism File is loaded at session start (no search) Semantic search on every prompt (embedding-based) Granularity One monolithic file, manually edited Per-session bullet points, automatically generated Search None -- Claude reads the whole file or nothing Hybrid semantic search (dense + BM25) returning top-k relevant chunks History depth Limited to what fits in one file Unlimited -- every session is logged, every entry is searchable Automatic capture <code>/memory</code> command requires manual intervention Fully automatic -- hooks capture every session Progressive disclosure None -- entire file is loaded into context 3-layer model (L1 auto-inject, L2 expand, L3 transcript) minimizes context usage Deduplication Manual -- user must avoid adding duplicates SHA-256 content hashing prevents duplicate embeddings Portability Tied to Claude Code's internal format Standard markdown files, usable with any tool"},{"location":"claude-plugin/#why-this-matters","title":"Why This Matters","text":"<p><code>CLAUDE.md</code> is a blunt instrument: it loads the entire file into context at session start, regardless of relevance. As the file grows, it wastes context window on irrelevant information and eventually hits size limits. There is no search -- Claude cannot selectively recall a specific decision from three weeks ago.</p> <p>memsearch solves this with semantic search and progressive disclosure. Instead of loading everything, it injects only the top-k most relevant memories for each specific prompt. History can grow indefinitely without degrading performance, because the vector index handles the filtering. And the three-layer model means Claude starts with lightweight previews and only drills deeper when needed, keeping context window usage minimal.</p>"},{"location":"claude-plugin/#plugin-files","title":"Plugin Files","text":"<p>The plugin lives in the <code>ccplugin/</code> directory at the root of the memsearch repository:</p> <pre><code>ccplugin/\n\u251c\u2500\u2500 .claude-plugin/\n\u2502   \u2514\u2500\u2500 plugin.json              # Plugin manifest (name, version, description)\n\u2514\u2500\u2500 hooks/\n    \u251c\u2500\u2500 hooks.json               # Hook definitions (4 lifecycle hooks)\n    \u251c\u2500\u2500 common.sh                # Shared setup: env, PATH, memsearch detection, watch management\n    \u251c\u2500\u2500 session-start.sh         # Start watch + write session heading + inject memories &amp; tools\n    \u251c\u2500\u2500 user-prompt-submit.sh    # Semantic search on prompt -&gt; inject memories with chunk_hash\n    \u251c\u2500\u2500 stop.sh                  # Parse transcript -&gt; haiku summary -&gt; append to daily .md\n    \u251c\u2500\u2500 parse-transcript.sh      # Deterministic JSONL-to-text parser with truncation\n    \u2514\u2500\u2500 session-end.sh           # Stop watch process (cleanup)\n</code></pre>"},{"location":"claude-plugin/#file-descriptions","title":"File Descriptions","text":"File Purpose <code>plugin.json</code> Claude Code plugin manifest. Declares the plugin name (<code>memsearch</code>), version, and description. <code>hooks.json</code> Defines the 4 lifecycle hooks (SessionStart, UserPromptSubmit, Stop, SessionEnd) with their types, timeouts, and async flags. <code>common.sh</code> Shared shell library sourced by all hooks. Handles stdin JSON parsing, PATH setup, memsearch binary detection (prefers PATH, falls back to <code>uv run</code>), memory directory management, and the watch singleton (start/stop with PID file and orphan cleanup). <code>session-start.sh</code> SessionStart hook implementation. Starts the watcher, writes the session heading, reads recent memory files, runs a semantic search for recent context, and injects Memory Tools instructions. <code>user-prompt-submit.sh</code> UserPromptSubmit hook implementation. Extracts the user prompt, runs <code>memsearch search</code> with <code>--top-k 3 --json-output</code>, and formats results with <code>chunk_hash</code> for progressive disclosure. <code>stop.sh</code> Stop hook implementation. Extracts the transcript path, validates it, delegates parsing to <code>parse-transcript.sh</code>, calls Haiku for summarization, and appends the result with session anchors to the daily memory file. <code>parse-transcript.sh</code> Standalone transcript parser. Processes the last 200 lines of a JSONL transcript, truncates content to 500 characters, extracts tool call summaries, and skips file-history-snapshot entries. Used by <code>stop.sh</code>. <code>session-end.sh</code> SessionEnd hook implementation. Calls <code>stop_watch</code> to terminate the background watcher and clean up."},{"location":"claude-plugin/#the-memsearch-cli","title":"The <code>memsearch</code> CLI","text":"<p>The plugin is built entirely on the <code>memsearch</code> CLI -- every hook is a shell script calling <code>memsearch</code> subcommands. Here are the commands most relevant to the plugin:</p> Command Used By What It Does <code>search &lt;query&gt;</code> UserPromptSubmit hook Semantic search over indexed memories (<code>--top-k</code> for result count, <code>--json-output</code> for JSON) <code>watch &lt;paths&gt;</code> SessionStart hook Background watcher that auto-indexes on file changes (1500ms debounce) <code>index &lt;paths&gt;</code> Manual / rebuild One-shot index of markdown files (<code>--force</code> to re-index all) <code>expand &lt;chunk_hash&gt;</code> Agent (L2 disclosure) Show full markdown section around a chunk, with anchor metadata <code>transcript &lt;jsonl&gt;</code> Agent (L3 disclosure) Parse Claude Code JSONL transcript into readable conversation turns <code>config init</code> Quick Start Interactive config wizard for first-time setup <code>stats</code> Manual Show index statistics (collection size, chunk count) <code>reset</code> Manual Drop all indexed data (requires <code>--yes</code> to confirm) <p>For the full CLI reference, see the CLI Reference page.</p>"},{"location":"cli/","title":"CLI Reference","text":"<p>memsearch provides a command-line interface for indexing, searching, and managing semantic memory over markdown knowledge bases.</p> <pre><code>$ memsearch --version\nmemsearch, version 0.1.3\n\n$ memsearch --help\nUsage: memsearch [OPTIONS] COMMAND [ARGS]...\n\n  memsearch \u2014 semantic memory search for markdown knowledge bases.\n\nOptions:\n  --version  Show the version and exit.\n  --help     Show this message and exit.\n\nCommands:\n  compact     Compress stored memories into a summary.\n  config      Manage memsearch configuration.\n  expand      Expand a memory chunk to show full context.\n  index       Index markdown files from PATHS.\n  reset       Drop all indexed data.\n  search      Search indexed memory for QUERY.\n  stats       Show statistics about the index.\n  transcript  View original conversation turns from a JSONL transcript.\n  watch       Watch PATHS for markdown changes and auto-index.\n</code></pre>"},{"location":"cli/#command-summary","title":"Command Summary","text":"Command Description <code>memsearch index</code> Scan directories and index markdown files into the vector store <code>memsearch search</code> Semantic search across indexed chunks using natural language <code>memsearch watch</code> Monitor directories and auto-index on file changes <code>memsearch compact</code> Compress indexed chunks into an LLM-generated summary <code>memsearch expand</code> Progressive disclosure L2: show full section around a chunk \ud83d\udd0c <code>memsearch transcript</code> Progressive disclosure L3: view turns from a JSONL transcript \ud83d\udd0c <code>memsearch config</code> Initialize, view, and modify configuration <code>memsearch stats</code> Display index statistics (total chunk count) <code>memsearch reset</code> Drop all indexed data from the Milvus collection <p>\ud83d\udd0c Commands marked with \ud83d\udd0c are designed for the Claude Code plugin's progressive disclosure workflow, but work as standalone CLI tools too.</p>"},{"location":"cli/#memsearch-index","title":"<code>memsearch index</code>","text":"<p>Scan one or more directories (or files) and index all markdown files (<code>.md</code>, <code>.markdown</code>) into the Milvus vector store. Only new or changed chunks are embedded by default -- unchanged chunks are skipped. Chunks belonging to deleted files are automatically removed from the index.</p>"},{"location":"cli/#options","title":"Options","text":"Flag Short Default Description <code>PATHS</code> (required) One or more directories or files to index <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (<code>openai</code>, <code>google</code>, <code>voyage</code>, <code>ollama</code>, <code>local</code>) <code>--model</code> <code>-m</code> provider default Override the embedding model name <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token (for server or Zilliz Cloud) <code>--force</code> <code>false</code> Re-embed and re-index all chunks, even if unchanged"},{"location":"cli/#examples","title":"Examples","text":"<p>Index a single directory:</p> <pre><code>$ memsearch index ./memory/\nIndexed 42 chunks.\n</code></pre> <p>Index multiple directories with a specific embedding provider:</p> <pre><code>$ memsearch index ./memory/ ./notes/ --provider google\nIndexed 87 chunks.\n</code></pre> <p>Force re-index everything (ignores the content-hash dedup check):</p> <pre><code>$ memsearch index ./memory/ --force\nIndexed 42 chunks.\n</code></pre> <p>Connect to a remote Milvus server instead of the default local file:</p> <pre><code>$ memsearch index ./memory/ --milvus-uri http://localhost:19530\nIndexed 42 chunks.\n</code></pre> <p>Use a custom embedding model:</p> <pre><code>$ memsearch index ./memory/ --provider openai --model text-embedding-3-large\nIndexed 42 chunks.\n</code></pre>"},{"location":"cli/#notes","title":"Notes","text":"<ul> <li>Incremental by default. Each chunk is identified by a composite hash of its source file, line range, content hash, and embedding model. Only chunks with new IDs are embedded and stored.</li> <li>Stale cleanup. If a file that was previously indexed no longer exists on disk, its chunks are automatically deleted from the index during the next <code>index</code> run.</li> <li><code>--force</code> re-embeds everything. Use this when you switch embedding providers or models, since the same content will produce different vectors with a different model.</li> </ul>"},{"location":"cli/#memsearch-search","title":"<code>memsearch search</code>","text":"<p>Run a semantic search query against indexed chunks. Uses hybrid search (dense vector cosine similarity + BM25 full-text) with RRF reranking for best results.</p>"},{"location":"cli/#options_1","title":"Options","text":"Flag Short Default Description <code>QUERY</code> (required) Natural-language search query <code>--top-k</code> <code>-k</code> <code>5</code> Maximum number of results to return <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (must match the provider used at index time) <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token <code>--json-output</code> <code>-j</code> <code>false</code> Output results as JSON"},{"location":"cli/#examples_1","title":"Examples","text":"<p>Basic search:</p> <pre><code>$ memsearch search \"how to configure Redis caching\"\n\n--- Result 1 (score: 0.0328) ---\nSource: /home/user/docs/2026-01-15.md\nHeading: Redis Configuration\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry...\n\n--- Result 2 (score: 0.0326) ---\nSource: /home/user/docs/architecture.md\nHeading: Caching Layer\nWe use Redis as the primary caching backend...\n</code></pre> <p>Return more results:</p> <pre><code>$ memsearch search \"authentication flow\" --top-k 10\n</code></pre> <p>Output as JSON (useful for piping to <code>jq</code> or other tools):</p> <pre><code>$ memsearch search \"error handling\" --json-output\n[\n  {\n    \"content\": \"All API endpoints should return structured error...\",\n    \"source\": \"/home/user/docs/api-design.md\",\n    \"heading\": \"Error Handling\",\n    \"chunk_hash\": \"a1b2c3d4e5f6...\",\n    \"heading_level\": 2,\n    \"start_line\": 45,\n    \"end_line\": 62,\n    \"score\": 0.0330\n  }\n]\n</code></pre> <p>Use with a different provider (must match the one used for indexing):</p> <pre><code>$ memsearch search \"database migrations\" --provider google\n</code></pre>"},{"location":"cli/#notes_1","title":"Notes","text":"<ul> <li>Provider must match. The search embedding provider and model must match whatever was used during indexing. Mixing providers will return poor results because the vector spaces are incompatible.</li> <li>Hybrid search. Results are ranked using Reciprocal Rank Fusion (RRF) across both dense (cosine) and sparse (BM25) retrieval, giving you the best of semantic and keyword matching.</li> <li>Content is truncated. In the default text output, each result's content is truncated to 500 characters. Use <code>--json-output</code> to get the full content.</li> </ul>"},{"location":"cli/#memsearch-watch","title":"<code>memsearch watch</code>","text":"<p>Start a long-running file watcher that monitors directories for markdown file changes. On startup, all existing markdown files are indexed first (dedup ensures no wasted API calls for unchanged content). Then the watcher monitors for changes: when a <code>.md</code> or <code>.markdown</code> file is created or modified, it is automatically re-indexed. When a file is deleted, its chunks are removed from the store.</p>"},{"location":"cli/#options_2","title":"Options","text":"Flag Short Default Description <code>PATHS</code> (required) One or more directories to watch <code>--debounce-ms</code> <code>1500</code> Debounce delay in milliseconds; multiple rapid changes to the same file within this window are collapsed into a single re-index <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_2","title":"Examples","text":"<p>Watch a single directory:</p> <pre><code>$ memsearch watch ./memory/\nIndexed 8 chunks.\nWatching 1 path(s) for changes... (Ctrl+C to stop)\nIndexed 3 chunks from /home/user/docs/2026-02-11.md\nRemoved chunks for /home/user/docs/old-draft.md\n^C\nStopping watcher.\n</code></pre> <p>Watch multiple directories with a longer debounce:</p> <pre><code>$ memsearch watch ./memory/ ./notes/ --debounce-ms 3000\nWatching 2 path(s) for changes... (Ctrl+C to stop)\n</code></pre>"},{"location":"cli/#notes_2","title":"Notes","text":"<ul> <li>Initial index on startup. The watcher indexes all existing files before it starts monitoring. Content-hash dedup means unchanged files are skipped with zero API calls \u2014 only genuinely new or modified content is embedded.</li> <li>Debounce. Editors that write files in multiple steps (e.g., write temp file, then rename) can trigger several events in quick succession. The debounce window collapses these into one re-index operation.</li> <li>Recursive. The watcher monitors all subdirectories recursively.</li> <li>Singleton behavior. Only one watcher process should run per directory set. Running multiple watchers on the same paths will cause duplicate indexing work (though dedup by content hash means the index stays consistent).</li> <li>Stop with Ctrl+C. The watcher runs until you interrupt it.</li> </ul>"},{"location":"cli/#memsearch-compact","title":"<code>memsearch compact</code>","text":"<p>Use an LLM to compress all indexed chunks (or a subset) into a condensed markdown summary. The summary is appended to a daily log file at <code>memory/YYYY-MM-DD.md</code> inside the first configured path, keeping markdown as the single source of truth.</p>"},{"location":"cli/#options_3","title":"Options","text":"Flag Short Default Description <code>--source</code> <code>-s</code> (all chunks) Only compact chunks from this specific source file <code>--output-dir</code> <code>-o</code> first configured path Directory to write the compact summary into <code>--llm-provider</code> <code>openai</code> LLM backend for summarization (<code>openai</code>, <code>anthropic</code>, <code>gemini</code>) <code>--llm-model</code> provider default Override the LLM model <code>--prompt</code> built-in template Custom prompt template string (must contain <code>{chunks}</code> placeholder) <code>--prompt-file</code> (none) Read the prompt template from a file instead <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider (used to access the index) <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#default-llm-models","title":"Default LLM Models","text":"Provider Default Model <code>openai</code> <code>gpt-4o-mini</code> <code>anthropic</code> <code>claude-sonnet-4-5-20250929</code> <code>gemini</code> <code>gemini-2.0-flash</code>"},{"location":"cli/#examples_3","title":"Examples","text":"<p>Compact all chunks using the default LLM (OpenAI):</p> <pre><code>$ memsearch compact\nCompact complete. Summary:\n\n## Key Decisions\n- Use Redis for session caching with 5-minute TTL\n- All API errors return structured JSON responses\n...\n</code></pre> <p>Compact only chunks from a specific source file:</p> <pre><code>$ memsearch compact --source ./memory/old-notes.md\nCompact complete. Summary:\n\n## Old Notes Summary\n- Initial architecture decisions from January meeting...\n</code></pre> <p>Use Anthropic Claude for summarization:</p> <pre><code>$ memsearch compact --llm-provider anthropic\n</code></pre> <p>Use a custom prompt template:</p> <pre><code>$ memsearch compact --prompt \"Summarize these notes into action items:\\n{chunks}\"\n</code></pre> <p>Use a prompt file for complex templates:</p> <pre><code>$ memsearch compact --prompt-file ./prompts/compress.txt\n</code></pre>"},{"location":"cli/#notes_3","title":"Notes","text":"<ul> <li>Output location. The summary is appended to <code>&lt;first-path&gt;/memory/YYYY-MM-DD.md</code>. This file is then automatically eligible for future indexing.</li> <li>The <code>{chunks}</code> placeholder is required. Whether using <code>--prompt</code> or <code>--prompt-file</code>, the template must contain <code>{chunks}</code> which will be replaced with the concatenated chunk contents.</li> <li>API key required. The chosen LLM provider requires its corresponding API key in the environment (see Environment Variables).</li> </ul>"},{"location":"cli/#memsearch-expand","title":"<code>memsearch expand</code>","text":"<p>\ud83d\udd0c Claude Code plugin command. This command is part of the ccplugin's three-level progressive disclosure workflow (<code>search</code> \u2192 <code>expand</code> \u2192 <code>transcript</code>), but works as a standalone CLI tool for any memsearch index.</p> <p>Look up a chunk by its hash in the index and return the surrounding context from the original source markdown file. This is \"progressive disclosure level 2\" -- when a search result snippet is not enough, expand it to see the full heading section.</p>"},{"location":"cli/#options_4","title":"Options","text":"Flag Short Default Description <code>CHUNK_HASH</code> (required) The chunk hash (primary key) to look up <code>--section/--no-section</code> <code>--section</code> Show the full heading section (default behavior) <code>--lines</code> <code>-n</code> (full section) Instead of the full section, show N lines before and after the chunk <code>--json-output</code> <code>-j</code> <code>false</code> Output as JSON <code>--provider</code> <code>-p</code> <code>openai</code> Embedding provider <code>--model</code> <code>-m</code> provider default Override the embedding model <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_4","title":"Examples","text":"<p>Expand a chunk to see its full heading section:</p> <pre><code>$ memsearch expand a1b2c3d4e5f6\nSource: /home/user/docs/architecture.md (lines 10-35)\nHeading: Caching Layer\n\n## Caching Layer\n\nWe use Redis as the primary caching backend. All cache keys\nfollow the pattern `service:entity:id`.\n\n### Configuration\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry.\n...\n</code></pre> <p>Show only 5 lines of context around the chunk:</p> <pre><code>$ memsearch expand a1b2c3d4e5f6 --lines 5\nSource: /home/user/docs/architecture.md (lines 18-28)\nHeading: Caching Layer\n\nSet REDIS_URL in .env to point to your Redis instance.\nUse `cache.set(key, value, ttl=300)` for 5-minute expiry.\n...\n</code></pre> <p>Get JSON output (includes anchor metadata if present):</p> <pre><code>$ memsearch expand a1b2c3d4e5f6 --json-output\n{\n  \"chunk_hash\": \"a1b2c3d4e5f6\",\n  \"source\": \"/home/user/docs/architecture.md\",\n  \"heading\": \"Caching Layer\",\n  \"start_line\": 10,\n  \"end_line\": 35,\n  \"content\": \"## Caching Layer\\n\\nWe use Redis as the primary...\"\n}\n</code></pre>"},{"location":"cli/#notes_4","title":"Notes","text":"<ul> <li>Source file must exist. The <code>expand</code> command reads the original markdown file from disk. If the source file has been moved or deleted, the command will fail with an error.</li> <li>Anchor parsing. If the expanded text contains an HTML anchor comment in the format <code>&lt;!-- session:ID turn:ID transcript:PATH --&gt;</code>, the command parses it and displays the session, turn, and transcript file information. This connects memory chunks to their original conversation transcripts.</li> <li>Workflow: search then expand. A typical workflow is to <code>search</code> first, note the <code>chunk_hash</code> from a result, then <code>expand</code> it to see more context.</li> </ul>"},{"location":"cli/#memsearch-transcript","title":"<code>memsearch transcript</code>","text":"<p>\ud83d\udd0c Claude Code plugin command. This command is part of the ccplugin's three-level progressive disclosure workflow (<code>search</code> \u2192 <code>expand</code> \u2192 <code>transcript</code>), but works as a standalone CLI tool for any JSONL transcript.</p> <p>Parse a JSONL transcript file (e.g., from Claude Code) and display conversation turns. This is \"progressive disclosure level 3\" -- drill all the way down from a memory chunk to the original conversation that generated it.</p>"},{"location":"cli/#options_5","title":"Options","text":"Flag Short Default Description <code>JSONL_PATH</code> (required) Path to the JSONL transcript file <code>--turn</code> <code>-t</code> (show all) Target turn UUID (prefix match supported) <code>--context</code> <code>-c</code> <code>3</code> Number of turns to show before and after the target turn <code>--json-output</code> <code>-j</code> <code>false</code> Output as JSON"},{"location":"cli/#examples_5","title":"Examples","text":"<p>List all turns in a transcript:</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl\nAll turns (12):\n\n  a1b2c3d4e5f6  14:23:05  Show me the Redis configuration code\n  d4e5f6a1b2c3  14:23:42  Can you add TTL support to the cache?\n  f6a1b2c3d4e5  14:25:10  Write tests for the cache module\n  ...\n</code></pre> <p>Show context around a specific turn (prefix match on UUID):</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl --turn d4e5f6\nShowing 5 turns around d4e5f6a1b2c3:\n\n[14:22:30] a1b2c3d4\nShow me the Redis configuration code\n\n**Assistant**: Here is the current Redis configuration...\n\n&gt;&gt;&gt; [14:23:42] d4e5f6a1\nCan you add TTL support to the cache?\n\n**Assistant**: I'll add TTL support. Here are the changes...\n  Tools: Edit(/src/cache.py), Bash(pytest tests/)\n\n[14:25:10] f6a1b2c3\nWrite tests for the cache module\n</code></pre> <p>Output as JSON:</p> <pre><code>$ memsearch transcript ./transcripts/session-abc123.jsonl --turn d4e5f6 --json-output\n[\n  {\n    \"uuid\": \"a1b2c3d4-...\",\n    \"timestamp\": \"2026-02-10T14:22:30Z\",\n    \"content\": \"Show me the Redis configuration code\\n\\n**Assistant**: ...\",\n    \"tool_calls\": []\n  }\n]\n</code></pre>"},{"location":"cli/#notes_5","title":"Notes","text":"<ul> <li>UUID prefix matching. You do not need to provide the full UUID. The first 6-8 characters are usually enough to uniquely identify a turn.</li> <li>The <code>&gt;&gt;&gt;</code> marker in text output highlights the target turn when using <code>--turn</code>.</li> <li>Three-level progressive disclosure workflow: <code>search</code> (L1: chunk snippet) -&gt; <code>expand</code> (L2: full section) -&gt; <code>transcript</code> (L3: original conversation).</li> </ul>"},{"location":"cli/#memsearch-config","title":"<code>memsearch config</code>","text":"<p>Manage memsearch configuration. Configuration is stored in TOML files and follows a layered priority chain:</p> <pre><code>dataclass defaults -&gt; ~/.memsearch/config.toml -&gt; .memsearch.toml -&gt; CLI flags\n</code></pre> <p>Higher-priority sources override lower-priority ones.</p>"},{"location":"cli/#subcommands","title":"Subcommands","text":""},{"location":"cli/#memsearch-config-init","title":"<code>memsearch config init</code>","text":"<p>Launch an interactive wizard that walks through all configuration sections and writes a TOML config file.</p> Flag Default Description <code>--project</code> <code>false</code> Write to <code>.memsearch.toml</code> (project-level) instead of the global config <pre><code>$ memsearch config init\nmemsearch configuration wizard\nWriting to: /home/user/.memsearch/config.toml\n\n-- Milvus --\n  Milvus URI [~/.memsearch/milvus.db]:\n  Milvus token (empty for none) []:\n  Collection name [memsearch_chunks]:\n\n-- Embedding --\n  Provider (openai/google/voyage/ollama/local) [openai]:\n  Model (empty for provider default) []:\n\n-- Chunking --\n  Max chunk size (chars) [1500]:\n  Overlap lines [2]:\n\n-- Watch --\n  Debounce (ms) [1500]:\n\n-- Compact --\n  LLM provider [openai]:\n  LLM model (empty for default) []:\n  Prompt file path (empty for built-in) []:\n\nConfig saved to /home/user/.memsearch/config.toml\n</code></pre> <p>Create a project-level config:</p> <pre><code>$ memsearch config init --project\nmemsearch configuration wizard\nWriting to: .memsearch.toml\n...\n</code></pre>"},{"location":"cli/#memsearch-config-set","title":"<code>memsearch config set</code>","text":"<p>Set a single configuration value by dotted key. Keys follow the <code>section.field</code> format.</p> Flag Default Description <code>KEY</code> (required) Dotted config key (e.g., <code>milvus.uri</code>) <code>VALUE</code> (required) Value to set <code>--project</code> <code>false</code> Write to <code>.memsearch.toml</code> instead of global config <pre><code>$ memsearch config set milvus.uri http://localhost:19530\nSet milvus.uri = http://localhost:19530 in /home/user/.memsearch/config.toml\n\n$ memsearch config set embedding.provider google --project\nSet embedding.provider = google in .memsearch.toml\n\n$ memsearch config set chunking.max_chunk_size 2000\nSet chunking.max_chunk_size = 2000 in /home/user/.memsearch/config.toml\n</code></pre>"},{"location":"cli/#memsearch-config-get","title":"<code>memsearch config get</code>","text":"<p>Read a single resolved configuration value (merged from all sources).</p> <pre><code>$ memsearch config get milvus.uri\nhttp://localhost:19530\n\n$ memsearch config get embedding.provider\nopenai\n\n$ memsearch config get chunking.max_chunk_size\n1500\n</code></pre>"},{"location":"cli/#memsearch-config-list","title":"<code>memsearch config list</code>","text":"<p>Display configuration in TOML format.</p> Flag Default Description <code>--resolved</code> (default) Show the fully merged configuration from all sources <code>--global</code> Show only the global config file (<code>~/.memsearch/config.toml</code>) <code>--project</code> Show only the project config file (<code>.memsearch.toml</code>) <pre><code>$ memsearch config list --resolved\n# Resolved (all sources merged)\n\n[milvus]\nuri = \"~/.memsearch/milvus.db\"\ntoken = \"\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"\nprompt_file = \"\"\n</code></pre> <pre><code>$ memsearch config list --global\n# Global (/home/user/.memsearch/config.toml)\n\n[milvus]\nuri = \"http://localhost:19530\"\n\n[embedding]\nprovider = \"openai\"\n</code></pre>"},{"location":"cli/#available-config-keys","title":"Available Config Keys","text":"Key Type Default Description <code>milvus.uri</code> string <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>milvus.token</code> string <code>\"\"</code> Auth token for Milvus Server / Zilliz Cloud <code>milvus.collection</code> string <code>memsearch_chunks</code> Collection name <code>embedding.provider</code> string <code>openai</code> Embedding provider name <code>embedding.model</code> string <code>\"\"</code> Override embedding model (empty = provider default) <code>chunking.max_chunk_size</code> int <code>1500</code> Maximum chunk size in characters <code>chunking.overlap_lines</code> int <code>2</code> Number of overlapping lines between adjacent chunks <code>watch.debounce_ms</code> int <code>1500</code> File watcher debounce delay in milliseconds <code>compact.llm_provider</code> string <code>openai</code> LLM provider for compact summarization <code>compact.llm_model</code> string <code>\"\"</code> Override LLM model (empty = provider default) <code>compact.prompt_file</code> string <code>\"\"</code> Path to a custom prompt template file"},{"location":"cli/#memsearch-stats","title":"<code>memsearch stats</code>","text":"<p>Show statistics about the current index, including the total number of stored chunks.</p>"},{"location":"cli/#options_6","title":"Options","text":"Flag Short Default Description <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token"},{"location":"cli/#examples_6","title":"Examples","text":"<pre><code>$ memsearch stats\nTotal indexed chunks: 142\n</code></pre> <p>Check stats for a specific collection on a remote server:</p> <pre><code>$ memsearch stats --milvus-uri http://localhost:19530 --collection my_project\nTotal indexed chunks: 87\n</code></pre>"},{"location":"cli/#notes_6","title":"Notes","text":"<ul> <li>Stats may lag on remote Milvus Server. The <code>get_collection_stats()</code> API on a remote Milvus Server may return stale counts immediately after an upsert. Stats are updated after segment flush and compaction. Search results are always up to date.</li> </ul>"},{"location":"cli/#memsearch-reset","title":"<code>memsearch reset</code>","text":"<p>Drop the entire Milvus collection, permanently deleting all indexed chunks. A confirmation prompt is shown before proceeding.</p>"},{"location":"cli/#options_7","title":"Options","text":"Flag Short Default Description <code>--collection</code> <code>-c</code> <code>memsearch_chunks</code> Milvus collection name <code>--milvus-uri</code> <code>~/.memsearch/milvus.db</code> Milvus connection URI <code>--milvus-token</code> (none) Milvus auth token <code>--yes</code> <code>-y</code> Skip the confirmation prompt"},{"location":"cli/#examples_7","title":"Examples","text":"<pre><code>$ memsearch reset\nThis will delete all indexed data. Continue? [y/N]: y\nDropped collection.\n</code></pre> <p>Skip the confirmation prompt (useful in scripts):</p> <pre><code>$ memsearch reset --yes\nDropped collection.\n</code></pre> <p>Reset a specific collection on a remote server:</p> <pre><code>$ memsearch reset --milvus-uri http://localhost:19530 --collection old_project --yes\nDropped collection.\n</code></pre>"},{"location":"cli/#notes_7","title":"Notes","text":"<ul> <li>This is destructive and irreversible. All indexed data will be lost. Your original markdown files are not affected -- you can always re-index them with <code>memsearch index</code>.</li> <li>Only drops the collection, not the database. If you are using Milvus Lite (a local <code>.db</code> file), the file itself remains; only the collection inside it is removed.</li> </ul>"},{"location":"cli/#environment-variables","title":"Environment Variables","text":"<p>memsearch reads API keys from environment variables. These are required by the corresponding embedding and LLM provider SDKs and are not stored in memsearch config files.</p>"},{"location":"cli/#api-keys","title":"API Keys","text":"Variable Required By Description <code>OPENAI_API_KEY</code> <code>openai</code> embedding provider, <code>openai</code> LLM compact provider OpenAI API key <code>OPENAI_BASE_URL</code> (optional) Override the OpenAI API base URL (for proxies or compatible APIs) <code>GOOGLE_API_KEY</code> <code>google</code> embedding provider, <code>gemini</code> LLM compact provider Google AI API key <code>VOYAGE_API_KEY</code> <code>voyage</code> embedding provider Voyage AI API key <code>ANTHROPIC_API_KEY</code> <code>anthropic</code> LLM compact provider Anthropic API key <code>OLLAMA_HOST</code> <code>ollama</code> embedding provider (optional) Ollama server URL (default: <code>http://localhost:11434</code>) <p>All memsearch settings (Milvus URI, embedding provider, chunking parameters, etc.) are configured via TOML config files or CLI flags -- see Configuration for details.</p>"},{"location":"cli/#examples_8","title":"Examples","text":"<pre><code># Set API key and run a search\n$ export OPENAI_API_KEY=sk-...\n$ memsearch search \"database schema\"\n\n# Use Google for embedding, Anthropic for compact\n$ export GOOGLE_API_KEY=AIza...\n$ memsearch index ./memory/ --provider google\n$ memsearch compact --llm-provider anthropic\n</code></pre>"},{"location":"cli/#embedding-provider-reference","title":"Embedding Provider Reference","text":"Provider Install Default Model Dimension API Key Variable <code>openai</code> included by default <code>text-embedding-3-small</code> 1536 <code>OPENAI_API_KEY</code> <code>google</code> <code>pip install \"memsearch[google]\"</code> <code>gemini-embedding-001</code> 768 <code>GOOGLE_API_KEY</code> <code>voyage</code> <code>pip install \"memsearch[voyage]\"</code> <code>voyage-3-lite</code> 512 <code>VOYAGE_API_KEY</code> <code>ollama</code> <code>pip install \"memsearch[ollama]\"</code> <code>nomic-embed-text</code> 768 (none, local) <code>local</code> <code>pip install \"memsearch[local]\"</code> <code>all-MiniLM-L6-v2</code> 384 (none, local) <p>Install all optional providers at once:</p> <pre><code>$ pip install \"memsearch[all]\"\n</code></pre>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install memsearch with pip (OpenAI embeddings are included by default):</p> <pre><code>$ pip install memsearch\n</code></pre>"},{"location":"getting-started/#extras-for-additional-embedding-providers","title":"Extras for additional embedding providers","text":"<p>Each optional extra pulls in the provider SDK you need:</p> <pre><code>$ pip install \"memsearch[google]\"      # Google Gemini embeddings\n$ pip install \"memsearch[voyage]\"      # Voyage AI embeddings\n$ pip install \"memsearch[ollama]\"      # Ollama (local, no API key)\n$ pip install \"memsearch[local]\"       # sentence-transformers (local, no API key)\n$ pip install \"memsearch[anthropic]\"   # Anthropic (for compact/summarization LLM)\n$ pip install \"memsearch[all]\"         # Everything above\n</code></pre>"},{"location":"getting-started/#how-it-all-fits-together","title":"How It All Fits Together","text":"<p>The diagram below shows the full lifecycle: writing markdown, indexing chunks, and searching them later.</p> <pre><code>sequenceDiagram\n    participant U as Your App\n    participant M as MemSearch\n    participant E as Embedding API\n    participant V as Milvus\n\n    U-&gt;&gt;M: save_memory(\"Redis config...\")\n    U-&gt;&gt;M: mem.index()\n    M-&gt;&gt;M: Chunk markdown\n    M-&gt;&gt;M: SHA-256 dedup\n    M-&gt;&gt;E: Embed new chunks\n    E--&gt;&gt;M: Vectors\n    M-&gt;&gt;V: Upsert\n    U-&gt;&gt;M: mem.search(\"Redis?\")\n    M-&gt;&gt;E: Embed query\n    E--&gt;&gt;M: Query vector\n    M-&gt;&gt;V: Cosine similarity\n    V--&gt;&gt;M: Top-K matches\n    M--&gt;&gt;U: Results with source info</code></pre> <p>Markdown is the source of truth. The vector store is a derived index -- rebuildable anytime from the original <code>.md</code> files. This means your memory is human-readable, <code>git</code>-friendly, and never locked into a proprietary format.</p>"},{"location":"getting-started/#your-first-memory-search","title":"Your First Memory Search","text":"<p>This section walks through the complete flow: create a memory directory, write some markdown files, index them, and search.</p>"},{"location":"getting-started/#set-up-your-memory-directory","title":"Set up your memory directory","text":"<p>memsearch follows the OpenClaw memory layout: a <code>MEMORY.md</code> file for persistent facts, plus daily logs in a <code>memory/</code> subdirectory.</p> <pre><code>$ mkdir -p my-project/memory\n$ cd my-project\n</code></pre> <p>Write a <code>MEMORY.md</code> with long-lived facts:</p> <pre><code>$ cat &gt; MEMORY.md &lt;&lt; 'EOF'\n# MEMORY.md\n\n## Team\n- Alice: frontend lead, React expert\n- Bob: backend lead, Python/FastAPI\n- Charlie: DevOps, manages Kubernetes\n\n## Architecture Decisions\n- ADR-001: Use event-driven architecture with Kafka\n- ADR-002: PostgreSQL 16 as primary database\n- ADR-003: Redis 7 for caching and sessions\n- ADR-004: Milvus for product semantic search\nEOF\n</code></pre> <p>Write a daily log:</p> <pre><code>$ cat &gt; memory/2026-02-10.md &lt;&lt; 'EOF'\n# 2026-02-10\n\n## Standup Notes\n- Alice finished the checkout redesign, merging today\n- Bob fixed the N+1 query in the order service \u2014 response time dropped from 800ms to 120ms\n- Charlie set up staging auto-deploy via GitHub Actions\n\n## Decision\nWe decided to migrate from REST to gRPC for inter-service communication.\nThe main drivers: type safety, streaming support, and ~40% latency reduction in benchmarks.\nEOF\n</code></pre>"},{"location":"getting-started/#index-with-the-cli","title":"Index with the CLI","text":"<pre><code>$ export OPENAI_API_KEY=\"sk-...\"\n$ memsearch index .\nIndexed 8 chunks.\n</code></pre>"},{"location":"getting-started/#search-with-the-cli","title":"Search with the CLI","text":"<pre><code>$ memsearch search \"what caching solution are we using?\"\n--- Result 1 (score: 0.0332) ---\nSource: MEMORY.md\nHeading: Architecture Decisions\n- ADR-003: Redis 7 for caching and sessions\n\n$ memsearch search \"what did Bob work on recently?\" --top-k 3\n--- Result 1 (score: 0.0328) ---\nSource: memory/2026-02-10.md\nHeading: Standup Notes\n- Bob fixed the N+1 query in the order service \u2014 response time dropped from 800ms to 120ms\n</code></pre> <p>Use <code>--json-output</code> to get structured results for piping into other tools:</p> <pre><code>$ memsearch search \"inter-service communication\" --json-output | python -m json.tool\n</code></pre>"},{"location":"getting-started/#search-with-the-python-api","title":"Search with the Python API","text":"<p>The same workflow in Python:</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\n\nasync def main():\n    mem = MemSearch(paths=[\".\"])\n    await mem.index()\n\n    results = await mem.search(\"what caching solution are we using?\", top_k=3)\n    for r in results:\n        print(f\"[{r['score']:.4f}] {r['source']} \u2014 {r['heading']}\")\n        print(f\"  {r['content'][:200]}\\n\")\n\n    mem.close()\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#building-an-agent-with-memory","title":"Building an Agent with Memory","text":"<p>The real power of memsearch is giving an LLM agent persistent memory across conversations. The pattern is simple: recall, think, remember.</p> <ol> <li>Recall -- search past memories for context relevant to the user's question</li> <li>Think -- call the LLM with that context injected into the system prompt</li> <li>Remember -- save the exchange to a daily markdown log and re-index</li> </ol>"},{"location":"getting-started/#openai-example-default","title":"OpenAI example (default)","text":"<pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = OpenAI()\nmem = MemSearch(paths=[MEMORY_DIR])\n\n\ndef save_memory(content: str):\n    \"\"\"Append a note to today's memory log (OpenClaw-style daily markdown).\"\"\"\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        if p.stat().st_size == 0:\n            f.write(f\"# {date.today()}\\n\")\n        f.write(f\"\\n{content}\\n\")\n\n\nasync def agent_chat(user_input: str) -&gt; str:\n    # 1. Recall \u2014 search past memories for relevant context\n    memories = await mem.search(user_input, top_k=5)\n    context = \"\\n\".join(f\"- {m['content'][:300]}\" for m in memories)\n\n    # 2. Think \u2014 call LLM with memory context\n    resp = llm.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\n                \"role\": \"system\",\n                \"content\": (\n                    \"You are a helpful assistant with access to the user's memory.\\n\"\n                    f\"Relevant memories:\\n{context}\"\n                ),\n            },\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.choices[0].message.content\n\n    # 3. Remember \u2014 save this exchange and re-index\n    save_memory(f\"## User: {user_input}\\n\\n{answer}\")\n    await mem.index()\n\n    return answer\n\n\nasync def main():\n    # Seed some knowledge\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    save_memory(\"## Decision\\nWe chose Redis for caching over Memcached.\")\n    await mem.index()\n\n    # Agent can now recall those memories\n    print(await agent_chat(\"Who is our frontend lead?\"))\n    print(await agent_chat(\"What caching solution did we pick?\"))\n\n\nasyncio.run(main())\n</code></pre>"},{"location":"getting-started/#anthropic-claude-variant","title":"Anthropic Claude variant","text":"<p>Install the Anthropic extra:</p> <pre><code>$ pip install \"memsearch[anthropic]\"\n</code></pre> <p>Then swap the LLM call:</p> <pre><code>from anthropic import Anthropic\n\nllm = Anthropic()\n\n# In agent_chat(), replace the OpenAI call with:\nresp = llm.messages.create(\n    model=\"claude-sonnet-4-5-20250929\",\n    max_tokens=1024,\n    system=f\"You have these memories:\\n{context}\",\n    messages=[{\"role\": \"user\", \"content\": user_input}],\n)\nanswer = resp.content[0].text\n</code></pre>"},{"location":"getting-started/#ollama-variant-fully-local-no-api-key","title":"Ollama variant (fully local, no API key)","text":"<pre><code>$ pip install \"memsearch[ollama]\"\n$ ollama pull nomic-embed-text    # embedding model\n$ ollama pull llama3.2            # chat model\n</code></pre> <pre><code>from ollama import chat\nfrom memsearch import MemSearch\n\n# Use Ollama for embeddings too \u2014 everything stays local\nmem = MemSearch(paths=[MEMORY_DIR], embedding_provider=\"ollama\")\n\n# In agent_chat(), replace the LLM call with:\nresp = chat(\n    model=\"llama3.2\",\n    messages=[\n        {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n        {\"role\": \"user\", \"content\": user_input},\n    ],\n)\nanswer = resp.message.content\n</code></pre>"},{"location":"getting-started/#api-keys","title":"API Keys","text":"<p>Set the environment variable for your chosen embedding provider. memsearch reads standard SDK environment variables -- no custom key names.</p> Provider Env Var Notes OpenAI (default) <code>OPENAI_API_KEY</code> Included with base install OpenAI-compatible proxy <code>OPENAI_BASE_URL</code> For Azure OpenAI, vLLM, LiteLLM, etc. Google Gemini <code>GOOGLE_API_KEY</code> Requires <code>memsearch[google]</code> Voyage AI <code>VOYAGE_API_KEY</code> Requires <code>memsearch[voyage]</code> Ollama <code>OLLAMA_HOST</code> (optional) Defaults to <code>http://localhost:11434</code> Local (sentence-transformers) -- No API key needed Anthropic <code>ANTHROPIC_API_KEY</code> Used by <code>compact</code> summarization only <pre><code>$ export OPENAI_API_KEY=\"sk-...\"         # OpenAI embeddings (default)\n$ export GOOGLE_API_KEY=\"...\"            # Google Gemini embeddings\n$ export VOYAGE_API_KEY=\"...\"            # Voyage AI embeddings\n$ export ANTHROPIC_API_KEY=\"...\"         # Anthropic (for compact summarization)\n</code></pre>"},{"location":"getting-started/#milvus-backends","title":"Milvus Backends","text":"<p>memsearch works with three Milvus deployment modes. Choose based on your needs:</p> <pre><code>graph TD\n    A[memsearch] --&gt; B{Choose backend}\n    B --&gt;|\"Default&lt;br&gt;(zero config)\"| C[\"Milvus Lite&lt;br&gt;~/.memsearch/milvus.db\"]\n    B --&gt;|\"Self-hosted&lt;br&gt;(multi-agent)\"| D[\"Milvus Server&lt;br&gt;localhost:19530\"]\n    B --&gt;|\"Managed&lt;br&gt;(production)\"| E[\"Zilliz Cloud&lt;br&gt;cloud.zilliz.com\"]\n\n    style C fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style D fill:#2a3a5c,stroke:#6ba3d6,color:#a8b2c1\n    style E fill:#2a3a5c,stroke:#e0976b,color:#a8b2c1</code></pre>"},{"location":"getting-started/#milvus-lite-default-zero-config","title":"Milvus Lite (default -- zero config)","text":"<p>Data is stored in a single local <code>.db</code> file. No server to install, no ports to open.</p> <p>Best for: personal use, single-agent setups, prototyping, development.</p> PythonCLI <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"~/.memsearch/milvus.db\",  # default, can be omitted\n)\n</code></pre> <pre><code>$ memsearch index ./memory/\n# Uses ~/.memsearch/milvus.db by default\n</code></pre>"},{"location":"getting-started/#milvus-server-self-hosted","title":"Milvus Server (self-hosted)","text":"<p>Deploy Milvus via Docker or Kubernetes. Multiple agents and users can share the same server instance, each using a separate collection or database.</p> <p>Best for: team environments, multi-agent workloads, shared always-on vector store.</p> PythonCLIDocker <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"http://localhost:19530\",\n    milvus_token=\"root:Milvus\",    # default credentials\n)\n</code></pre> <pre><code>$ memsearch index ./memory/ --milvus-uri http://localhost:19530 --milvus-token root:Milvus\n</code></pre> <pre><code>$ docker run -d --name milvus \\\n    -p 19530:19530 -p 9091:9091 \\\n    milvusdb/milvus:latest milvus run standalone\n</code></pre>"},{"location":"getting-started/#zilliz-cloud-fully-managed","title":"Zilliz Cloud (fully managed)","text":"<p>Zero-ops, auto-scaling managed Milvus. Get a free cluster at cloud.zilliz.com.</p> <p>Best for: production deployments, teams that do not want to manage infrastructure.</p> PythonCLI <pre><code>mem = MemSearch(\n    paths=[\"./memory/\"],\n    milvus_uri=\"https://in03-xxx.api.gcp-us-west1.zillizcloud.com\",\n    milvus_token=\"your-api-key\",\n)\n</code></pre> <pre><code>$ memsearch index ./memory/ \\\n    --milvus-uri \"https://in03-xxx.api.gcp-us-west1.zillizcloud.com\" \\\n    --milvus-token \"your-api-key\"\n</code></pre>"},{"location":"getting-started/#configuration","title":"Configuration","text":"<p>memsearch uses a layered configuration system. Settings are resolved in priority order (lowest to highest):</p> <ol> <li>Built-in defaults -- sensible out-of-the-box values</li> <li>Global config -- <code>~/.memsearch/config.toml</code></li> <li>Project config -- <code>.memsearch.toml</code> in your working directory</li> <li>CLI flags -- <code>--milvus-uri</code>, <code>--provider</code>, etc.</li> </ol> <p>Higher-priority sources override lower ones. This means you can set defaults globally, customize per project, and override on the fly with CLI flags.</p> <p>Note: API keys for embedding and LLM providers (e.g. <code>OPENAI_API_KEY</code>, <code>GOOGLE_API_KEY</code>) are read from environment variables by their respective SDKs. They are not stored in memsearch config files. See API Keys below.</p>"},{"location":"getting-started/#interactive-config-wizard","title":"Interactive config wizard","text":"<p>The fastest way to configure memsearch:</p> <pre><code>$ memsearch config init\nmemsearch configuration wizard\nWriting to: /home/user/.memsearch/config.toml\n\n\u2500\u2500 Milvus \u2500\u2500\n  Milvus URI [~/.memsearch/milvus.db]:\n  Milvus token (empty for none) []:\n  Collection name [memsearch_chunks]:\n\n\u2500\u2500 Embedding \u2500\u2500\n  Provider (openai/google/voyage/ollama/local) [openai]:\n  Model (empty for provider default) []:\n\n\u2500\u2500 Chunking \u2500\u2500\n  Max chunk size (chars) [1500]:\n  Overlap lines [2]:\n...\n\nConfig saved to /home/user/.memsearch/config.toml\n</code></pre> <p>Use <code>--project</code> to write to <code>.memsearch.toml</code> in the current directory instead:</p> <pre><code>$ memsearch config init --project\n</code></pre>"},{"location":"getting-started/#config-file-locations","title":"Config file locations","text":"Scope Path Use case Global <code>~/.memsearch/config.toml</code> Machine-wide defaults (Milvus URI, preferred provider) Project <code>.memsearch.toml</code> Per-project overrides (collection name, custom model) <p>Both files use TOML format:</p> <pre><code># Example ~/.memsearch/config.toml\n\n[milvus]\nuri = \"http://localhost:19530\"\ntoken = \"root:Milvus\"\ncollection = \"memsearch_chunks\"\n\n[embedding]\nprovider = \"openai\"\nmodel = \"\"\n\n[chunking]\nmax_chunk_size = 1500\noverlap_lines = 2\n\n[watch]\ndebounce_ms = 1500\n\n[compact]\nllm_provider = \"openai\"\nllm_model = \"\"\nprompt_file = \"\"\n</code></pre>"},{"location":"getting-started/#get-and-set-individual-values","title":"Get and set individual values","text":"<pre><code>$ memsearch config set milvus.uri http://localhost:19530\nSet milvus.uri = http://localhost:19530 in /home/user/.memsearch/config.toml\n\n$ memsearch config get milvus.uri\nhttp://localhost:19530\n\n$ memsearch config set embedding.provider ollama --project\nSet embedding.provider = ollama in .memsearch.toml\n</code></pre>"},{"location":"getting-started/#view-resolved-configuration","title":"View resolved configuration","text":"<pre><code>$ memsearch config list --resolved    # Final merged config from all sources\n$ memsearch config list --global      # Show ~/.memsearch/config.toml only\n$ memsearch config list --project     # Show .memsearch.toml only\n</code></pre>"},{"location":"getting-started/#cli-flag-overrides","title":"CLI flag overrides","text":"<p>CLI flags always take the highest priority:</p> <pre><code>$ memsearch index ./memory/ --provider google --milvus-uri http://localhost:19530\n$ memsearch search \"Redis config\" --top-k 10 --milvus-uri http://10.0.0.5:19530\n</code></pre>"},{"location":"getting-started/#whats-next","title":"What's Next","text":"<ul> <li>Architecture -- deep dive into the chunking pipeline, dedup strategy, and data flow diagrams</li> <li>CLI Reference -- complete reference for all <code>memsearch</code> commands, flags, and options</li> <li>Claude Code Plugin -- give Claude automatic persistent memory across sessions with zero configuration</li> </ul>"},{"location":"integrations/","title":"Integrations","text":"<p>memsearch is a plain Python library -- it works with any framework. This page shows ready-made patterns for LangChain, LangGraph, LlamaIndex, and CrewAI.</p> <p>Prerequisites</p> <p>Each integration requires its own packages:</p> <pre><code>$ pip install langchain langchain-openai    # LangChain examples\n$ pip install langgraph                      # LangGraph agent example\n$ pip install llama-index-core              # LlamaIndex example\n$ pip install crewai                         # CrewAI example\n</code></pre>"},{"location":"integrations/#langchain","title":"LangChain","text":""},{"location":"integrations/#as-a-retriever","title":"As a Retriever","text":"<p>Wrap <code>MemSearch</code> in a LangChain <code>BaseRetriever</code> so it plugs into any LangChain chain or agent.</p> <pre><code>import asyncio\nfrom pydantic import ConfigDict\nfrom memsearch import MemSearch\nfrom langchain_core.retrievers import BaseRetriever\nfrom langchain_core.documents import Document\nfrom langchain_core.callbacks import CallbackManagerForRetrieverRun\n\n\nclass MemSearchRetriever(BaseRetriever):\n    \"\"\"LangChain retriever backed by memsearch.\"\"\"\n\n    mem: MemSearch\n    top_k: int = 5\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    def _get_relevant_documents(\n        self, query: str, *, run_manager: CallbackManagerForRetrieverRun\n    ) -&gt; list[Document]:\n        results = asyncio.run(self.mem.search(query, top_k=self.top_k))\n        return [\n            Document(\n                page_content=r[\"content\"],\n                metadata={\n                    \"source\": r[\"source\"],\n                    \"heading\": r[\"heading\"],\n                    \"score\": r[\"score\"],\n                },\n            )\n            for r in results\n        ]\n</code></pre> <p>Use it like any other LangChain retriever:</p> <pre><code>mem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\nretriever = MemSearchRetriever(mem=mem, top_k=3)\ndocs = retriever.invoke(\"Redis caching\")\n# [Document(page_content=\"We chose Redis for caching...\", metadata={...}), ...]\n</code></pre>"},{"location":"integrations/#rag-chain","title":"RAG Chain","text":"<p>Combine the retriever with an LLM using LCEL (LangChain Expression Language) for a simple retrieval-augmented generation pipeline:</p> <pre><code>from langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nretriever = MemSearchRetriever(mem=mem, top_k=3)\n\n\ndef format_docs(docs: list[Document]) -&gt; str:\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n\nprompt = ChatPromptTemplate.from_template(\n    \"Use the following context to answer the question.\\n\\n\"\n    \"Context:\\n{context}\\n\\n\"\n    \"Question: {question}\\n\"\n    \"Answer:\"\n)\n\nrag_chain = (\n    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n    | prompt\n    | llm\n    | StrOutputParser()\n)\n\nanswer = rag_chain.invoke(\"what caching solution are we using?\")\nprint(answer)\n</code></pre>"},{"location":"integrations/#langgraph","title":"LangGraph","text":""},{"location":"integrations/#as-a-tool-react-agent","title":"As a Tool (ReAct Agent)","text":"<p>Wrap memsearch as a tool and let a LangGraph ReAct agent decide when to search:</p> <pre><code>import asyncio\nfrom langchain_core.tools import tool\nfrom langchain_openai import ChatOpenAI\nfrom langgraph.prebuilt import create_react_agent\nfrom memsearch import MemSearch\n\nmem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\n\n@tool\ndef search_memory(query: str) -&gt; str:\n    \"\"\"Search the team's knowledge base for relevant information.\"\"\"\n    results = asyncio.run(mem.search(query, top_k=3))\n    if not results:\n        return \"No relevant memories found.\"\n    return \"\\n\\n\".join(\n        f\"[{r['source']}] {r['heading']}: {r['content'][:300]}\"\n        for r in results\n    )\n\n\nllm = ChatOpenAI(model=\"gpt-4o-mini\")\nagent = create_react_agent(llm, [search_memory])\n\nresult = agent.invoke(\n    {\"messages\": [(\"user\", \"Who is our frontend lead and what did they work on?\")]}\n)\n\n# The agent automatically calls search_memory when it needs information\nfor msg in result[\"messages\"]:\n    role = msg.__class__.__name__\n    if hasattr(msg, \"tool_calls\") and msg.tool_calls:\n        print(f\"{role}: [called search_memory]\")\n    elif hasattr(msg, \"content\") and msg.content:\n        print(f\"{role}: {msg.content[:200]}\")\n</code></pre> <p>The agent will autonomously decide when to call <code>search_memory</code> based on the user's question -- no manual retrieval logic needed.</p>"},{"location":"integrations/#llamaindex","title":"LlamaIndex","text":""},{"location":"integrations/#as-a-retriever_1","title":"As a Retriever","text":"<p>Implement a LlamaIndex <code>BaseRetriever</code> that delegates to memsearch. Results are returned as <code>NodeWithScore</code> objects that work with any LlamaIndex query engine or pipeline.</p> <pre><code>import asyncio\nfrom typing import List\nfrom memsearch import MemSearch\nfrom llama_index.core.retrievers import BaseRetriever\nfrom llama_index.core.schema import NodeWithScore, TextNode, QueryBundle\n\n\nclass MemSearchRetriever(BaseRetriever):\n    \"\"\"LlamaIndex retriever backed by memsearch.\"\"\"\n\n    def __init__(self, mem: MemSearch, top_k: int = 5) -&gt; None:\n        self._mem = mem\n        self._top_k = top_k\n        super().__init__()\n\n    def _retrieve(self, query_bundle: QueryBundle) -&gt; List[NodeWithScore]:\n        results = asyncio.run(\n            self._mem.search(query_bundle.query_str, top_k=self._top_k)\n        )\n        return [\n            NodeWithScore(\n                node=TextNode(\n                    text=r[\"content\"],\n                    metadata={\"source\": r[\"source\"], \"heading\": r[\"heading\"]},\n                ),\n                score=r[\"score\"],\n            )\n            for r in results\n        ]\n</code></pre> <p>Use it like any other LlamaIndex retriever:</p> <pre><code>mem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\nretriever = MemSearchRetriever(mem=mem, top_k=3)\nnodes = retriever.retrieve(\"Redis caching\")\nfor n in nodes:\n    print(f\"[{n.score:.4f}] {n.node.metadata['source']} \u2014 {n.node.text[:100]}\")\n</code></pre> <p>Plug it into a <code>RetrieverQueryEngine</code> for end-to-end RAG (requires an LLM provider like <code>llama-index-llms-openai</code>):</p> <pre><code>from llama_index.core.query_engine import RetrieverQueryEngine\n\nquery_engine = RetrieverQueryEngine.from_args(retriever)\nresponse = query_engine.query(\"what caching solution are we using?\")\nprint(response)\n</code></pre>"},{"location":"integrations/#crewai","title":"CrewAI","text":""},{"location":"integrations/#as-a-tool-multi-agent-crew","title":"As a Tool (Multi-Agent Crew)","text":"<p>Register memsearch as a CrewAI tool so any agent in the crew can search the knowledge base:</p> <pre><code>import asyncio\nfrom memsearch import MemSearch\nfrom crewai import Agent, Task, Crew\nfrom crewai.tools import tool\n\nmem = MemSearch(paths=[\"./memory/\"])\nasyncio.run(mem.index())\n\n\n@tool(\"search_memory\")\ndef search_memory(query: str) -&gt; str:\n    \"\"\"Search the team's knowledge base for relevant information.\"\"\"\n    results = asyncio.run(mem.search(query, top_k=3))\n    if not results:\n        return \"No relevant memories found.\"\n    return \"\\n\\n\".join(\n        f\"[{r['source']}] {r['heading']}: {r['content'][:300]}\"\n        for r in results\n    )\n\n\nresearcher = Agent(\n    role=\"Knowledge Base Researcher\",\n    goal=\"Find relevant information from the team's knowledge base\",\n    backstory=\"You are a researcher who searches the team's knowledge base to answer questions.\",\n    tools=[search_memory],\n)\n\nresearch_task = Task(\n    description=\"Who is the frontend lead and what did they work on recently?\",\n    expected_output=\"A short summary mentioning the frontend lead's name and recent work.\",\n    agent=researcher,\n)\n\ncrew = Crew(agents=[researcher], tasks=[research_task])\nresult = crew.kickoff()\nprint(result)\n</code></pre> <p>The agent will automatically call <code>search_memory</code> to look up the answer before responding.</p>"},{"location":"python-api/","title":"Python API","text":"<p>memsearch provides a high-level Python API through the <code>MemSearch</code> class. Import it, point it at your markdown files, and you get semantic memory for your agent in a few lines of code.</p> <pre><code>from memsearch import MemSearch\n\nmem = MemSearch(paths=[\"./memory\"])\n\nawait mem.index()                                      # index markdown files\nresults = await mem.search(\"Redis config\", top_k=3)    # semantic search\nprint(results[0][\"content\"], results[0][\"score\"])       # content + similarity\n</code></pre>"},{"location":"python-api/#memsearch","title":"<code>MemSearch</code>","text":"<p>The main entry point. Handles indexing, search, compaction, and file watching.</p>"},{"location":"python-api/#constructor","title":"Constructor","text":"<pre><code>MemSearch(\n    paths=[\"./memory\"],\n    *,\n    embedding_provider=\"openai\",\n    embedding_model=None,\n    milvus_uri=\"~/.memsearch/milvus.db\",\n    milvus_token=None,\n    collection=\"memsearch_chunks\",\n    max_chunk_size=1500,\n    overlap_lines=2,\n)\n</code></pre> Parameter Type Default Description <code>paths</code> <code>list[str \\| Path]</code> <code>[]</code> Directories or files to index <code>embedding_provider</code> <code>str</code> <code>\"openai\"</code> Embedding backend (<code>\"openai\"</code>, <code>\"google\"</code>, <code>\"voyage\"</code>, <code>\"ollama\"</code>, <code>\"local\"</code>) <code>embedding_model</code> <code>str \\| None</code> <code>None</code> Override the default model for the chosen provider <code>milvus_uri</code> <code>str</code> <code>\"~/.memsearch/milvus.db\"</code> Milvus connection URI \u2014 local <code>.db</code> path for Milvus Lite, <code>http://host:port</code> for Milvus Server, or <code>https://*.zillizcloud.com</code> for Zilliz Cloud <code>milvus_token</code> <code>str \\| None</code> <code>None</code> Auth token for Milvus Server or Zilliz Cloud <code>collection</code> <code>str</code> <code>\"memsearch_chunks\"</code> Milvus collection name. Use different names to isolate agents sharing the same backend <code>max_chunk_size</code> <code>int</code> <code>1500</code> Maximum chunk size in characters <code>overlap_lines</code> <code>int</code> <code>2</code> Overlapping lines between adjacent chunks"},{"location":"python-api/#context-manager","title":"Context Manager","text":"<p><code>MemSearch</code> implements the context manager protocol. Use <code>with</code> to ensure resources are released:</p> <pre><code>with MemSearch(paths=[\"./memory\"]) as mem:\n    await mem.index()\n    results = await mem.search(\"Redis config\")\n# Milvus connection is closed automatically\n</code></pre> <p>Or call <code>mem.close()</code> manually when done.</p>"},{"location":"python-api/#methods","title":"Methods","text":""},{"location":"python-api/#index","title":"<code>index</code>","text":"<pre><code>await mem.index(*, force=False) -&gt; int\n</code></pre> <p>Scan all configured paths and index every markdown file (<code>.md</code>, <code>.markdown</code>) into the vector store. Returns the number of chunks indexed.</p> Parameter Type Default Description <code>force</code> <code>bool</code> <code>False</code> Re-embed all chunks even if unchanged. Use this after switching embedding providers <p>Behavior:</p> <ul> <li>Incremental by default. Only new or changed chunks are embedded. Unchanged chunks are skipped via content-hash dedup.</li> <li>Stale cleanup. Chunks from deleted files are automatically removed.</li> <li>Deleted content. If a section is removed from a file, its old chunks are cleaned up on the next <code>index()</code> call.</li> </ul> <pre><code>mem = MemSearch(paths=[\"./memory\", \"./notes\"])\nn = await mem.index()\nprint(f\"Indexed {n} chunks\")\n\n# After switching to a different embedding provider, force re-index\nn = await mem.index(force=True)\n</code></pre>"},{"location":"python-api/#index_file","title":"<code>index_file</code>","text":"<pre><code>await mem.index_file(path) -&gt; int\n</code></pre> <p>Index a single file. Returns the number of chunks indexed.</p> Parameter Type Description <code>path</code> <code>str \\| Path</code> Path to a markdown file <pre><code>n = await mem.index_file(\"./memory/2026-02-12.md\")\n</code></pre>"},{"location":"python-api/#search","title":"<code>search</code>","text":"<pre><code>await mem.search(query, *, top_k=10) -&gt; list[dict]\n</code></pre> <p>Semantic search across indexed chunks. Returns a list of result dicts, sorted by relevance.</p> Parameter Type Default Description <code>query</code> <code>str</code> (required) Natural-language search query <code>top_k</code> <code>int</code> <code>10</code> Maximum number of results <p>Return value: Each dict contains:</p> Key Type Description <code>content</code> <code>str</code> The chunk text <code>source</code> <code>str</code> Path to the source markdown file <code>heading</code> <code>str</code> The heading this chunk belongs to <code>heading_level</code> <code>int</code> Heading level (1\u20136, or 0 for no heading) <code>chunk_hash</code> <code>str</code> Unique chunk identifier <code>start_line</code> <code>int</code> Start line in the source file <code>end_line</code> <code>int</code> End line in the source file <code>score</code> <code>float</code> Relevance score (higher is better) <pre><code>results = await mem.search(\"who is the frontend lead?\", top_k=5)\nfor r in results:\n    print(f\"[{r['score']:.4f}] {r['heading']}: {r['content'][:100]}\")\n</code></pre>"},{"location":"python-api/#compact","title":"<code>compact</code>","text":"<pre><code>await mem.compact(\n    *,\n    source=None,\n    llm_provider=\"openai\",\n    llm_model=None,\n    prompt_template=None,\n    output_dir=None,\n) -&gt; str\n</code></pre> <p>Use an LLM to compress indexed chunks into a summary. The summary is appended to <code>memory/YYYY-MM-DD.md</code> and automatically indexed.</p> Parameter Type Default Description <code>source</code> <code>str \\| None</code> <code>None</code> Only compact chunks from this source file. <code>None</code> = all chunks <code>llm_provider</code> <code>str</code> <code>\"openai\"</code> LLM backend (<code>\"openai\"</code>, <code>\"anthropic\"</code>, <code>\"gemini\"</code>) <code>llm_model</code> <code>str \\| None</code> <code>None</code> Override the default LLM model <code>prompt_template</code> <code>str \\| None</code> <code>None</code> Custom prompt (must contain <code>{chunks}</code> placeholder) <code>output_dir</code> <code>str \\| Path \\| None</code> <code>None</code> Where to write the summary. Defaults to the first configured path <p>Default LLM models:</p> Provider Default Model <code>openai</code> <code>gpt-4o-mini</code> <code>anthropic</code> <code>claude-sonnet-4-5-20250929</code> <code>gemini</code> <code>gemini-2.0-flash</code> <pre><code># Compact all memories\nsummary = await mem.compact()\nprint(summary)\n\n# Compact only one file, using Claude\nsummary = await mem.compact(\n    source=\"./memory/old-notes.md\",\n    llm_provider=\"anthropic\",\n)\n</code></pre>"},{"location":"python-api/#watch","title":"<code>watch</code>","text":"<pre><code>mem.watch(*, on_event=None, debounce_ms=None) -&gt; FileWatcher\n</code></pre> <p>Start a background file watcher that auto-indexes markdown changes. This is a synchronous method that returns a <code>FileWatcher</code> object running in a background thread.</p> Parameter Type Default Description <code>on_event</code> <code>Callable</code> <code>None</code> Callback invoked after each event: <code>(event_type, summary, file_path)</code>. <code>event_type</code> is <code>\"created\"</code>, <code>\"modified\"</code>, or <code>\"deleted\"</code> <code>debounce_ms</code> <code>int \\| None</code> <code>None</code> Debounce delay in milliseconds. Defaults to 1500 if not set <p>Returns: a <code>FileWatcher</code> instance. Call <code>watcher.stop()</code> to stop watching, or use it as a context manager.</p> <pre><code>mem = MemSearch(paths=[\"./memory\"])\nawait mem.index()  # initial index\n\n# Start watching for changes in the background\nwatcher = mem.watch(on_event=lambda t, s, p: print(f\"[{t}] {s}\"))\n\n# ... your agent runs here ...\n\nwatcher.stop()\n</code></pre>"},{"location":"python-api/#close","title":"<code>close</code>","text":"<pre><code>mem.close() -&gt; None\n</code></pre> <p>Release the Milvus connection and other resources. Called automatically when using <code>MemSearch</code> as a context manager.</p>"},{"location":"python-api/#full-example","title":"Full Example","text":"<p>A complete agent loop: seed knowledge, index it, then recall it during conversation.</p> OpenAIAnthropic ClaudeOllama (fully local) <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom openai import OpenAI\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = OpenAI()\nmem = MemSearch(paths=[MEMORY_DIR])\n\ndef save_memory(content: str):\n    \"\"\"Append a note to today's memory log.\"\"\"\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    # 1. Recall \u2014 search past memories\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    # 2. Think \u2014 call LLM with memory context\n    resp = llm.chat.completions.create(\n        model=\"gpt-4o-mini\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.choices[0].message.content\n\n    # 3. Remember \u2014 save and index\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    save_memory(\"## Decision\\nWe chose Redis for caching over Memcached.\")\n    await mem.index()  # or mem.watch() to auto-index in the background\n\n    print(await agent_chat(\"Who is our frontend lead?\"))\n    print(await agent_chat(\"What caching solution did we pick?\"))\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom anthropic import Anthropic\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nllm = Anthropic()\nmem = MemSearch(paths=[MEMORY_DIR])\n\ndef save_memory(content: str):\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    resp = llm.messages.create(\n        model=\"claude-sonnet-4-5-20250929\",\n        max_tokens=1024,\n        system=f\"You have these memories:\\n{context}\",\n        messages=[{\"role\": \"user\", \"content\": user_input}],\n    )\n    answer = resp.content[0].text\n\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    await mem.index()\n    print(await agent_chat(\"Who is our frontend lead?\"))\n\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\nfrom datetime import date\nfrom pathlib import Path\nfrom ollama import chat\nfrom memsearch import MemSearch\n\nMEMORY_DIR = \"./memory\"\nmem = MemSearch(paths=[MEMORY_DIR], embedding_provider=\"ollama\")\n\ndef save_memory(content: str):\n    p = Path(MEMORY_DIR) / f\"{date.today()}.md\"\n    p.parent.mkdir(parents=True, exist_ok=True)\n    with open(p, \"a\") as f:\n        f.write(f\"\\n{content}\\n\")\n\nasync def agent_chat(user_input: str) -&gt; str:\n    memories = await mem.search(user_input, top_k=3)\n    context = \"\\n\".join(f\"- {m['content'][:200]}\" for m in memories)\n\n    resp = chat(\n        model=\"llama3.2\",\n        messages=[\n            {\"role\": \"system\", \"content\": f\"You have these memories:\\n{context}\"},\n            {\"role\": \"user\", \"content\": user_input},\n        ],\n    )\n    answer = resp.message.content\n\n    save_memory(f\"## {user_input}\\n{answer}\")\n    await mem.index()\n    return answer\n\nasync def main():\n    save_memory(\"## Team\\n- Alice: frontend lead\\n- Bob: backend lead\")\n    await mem.index()\n    print(await agent_chat(\"Who is our frontend lead?\"))\n\nasyncio.run(main())\n</code></pre>"}]}